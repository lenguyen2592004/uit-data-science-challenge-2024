{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9627952,"sourceType":"datasetVersion","datasetId":5877210},{"sourceId":9646085,"sourceType":"datasetVersion","datasetId":5890979},{"sourceId":9835556,"sourceType":"datasetVersion","datasetId":6033072},{"sourceId":9850807,"sourceType":"datasetVersion","datasetId":5879213}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install open_clip_torch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install git+https://github.com/BloodAxe/pytorch-toolbelt.git\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torch==2.4.0 torchvision==0.19.0 --index-url https://download.pytorch.org/whl/cu124","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())\n\n#  True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport os\nimport torch\nimport re\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom transformers import AutoModel, AutoTokenizer, get_scheduler\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom torch.optim import AdamW,Adam\nfrom tqdm.notebook import tqdm, trange\nfrom time import perf_counter\nfrom PIL import Image\nimport pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!export CUDA_VISIBLE_DEVICES=0,1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set random seeds for repeatability\nimport numpy as np\nimport random\n\ndef set_seed(seed_val):\n    random.seed(seed_val)\n    np.random.seed(seed_val)\n    torch.manual_seed(seed_val)\n    torch.cuda.manual_seed_all(seed_val)\nseed_val = 0\nset_seed(seed_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndf=pd.read_csv('/kaggle/input/dsc2024/train_cluster.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nwith open('/kaggle/input/dsc2024/vimmsd-private-test.json', 'r') as f:\n    data = json.load(f)\n\n# S·ª≠ d·ª•ng json_normalize ƒë·ªÉ l√†m ph·∫≥ng d·ªØ li·ªáu trong \"root\"\ndf_test = pd.DataFrame.from_dict(data, orient='index')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_to_emoji(text):\n    emoji_map = {\n        \":))\": \"üòä\",\n        \"=)))\": \"üòä\",# Happy\n        \":((\": \"üòû\",  # Sad\n        \":'(\": \"üò¢\",  # Crying\n        \":D\": \"üòÉ\",   # Big smile\n        \":(\": \"‚òπÔ∏è\",   # Disappointed\n        \":|\": \"üòê\",   # Neutral\n    }\n    for text_emoji, real_emoji in emoji_map.items():\n        text = text.replace(text_emoji, real_emoji)\n    return text\n\n# Preprocessing function\ndef preprocess_text(text):\n    text = text.lower()\n    text = text_to_emoji(text)\n    text = re.sub(r'[^\\w\\s,]', '', text)\n    text = re.sub(r'\\d+', '', text)\n    text = ' '.join(text.split())\n    return text\n\ndf['caption']=df['caption'].apply(preprocess_text).apply(text_to_emoji)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# split train and dev\ndf_train, df_dev= train_test_split(df, test_size=0.1, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMAGE_TRAIN_FOLDER='/kaggle/input/dsc2024/training-images/train-images/'\nIMAGE_TEST_FOLDER='/kaggle/input/dsc2024/private-test-images/test-images/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef show_sample(row_num):\n    sample_row = df_train.iloc[row_num]\n    print('Index:', row_num)\n    print('Label:', sample_row['label'])\n    print('Text:', sample_row['caption'])\n    image_path = IMAGE_TRAIN_FOLDER + sample_row['image']\n    im = Image.open(image_path)\n    plt.imshow(im)\n    plt.axis('off')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from random import randint\nshow_sample(randint(0, len(df_train)-1))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_to_id = {lab:i for i, lab in enumerate(df_train['label'].sort_values().unique())}\nid_to_label = {v:k for k,v in label_to_id.items()}\nlabel_to_id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_out_labels = len(label_to_id)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport pandas as pd\nfrom PIL import Image\n\nclass MultimodalDataset(Dataset):\n    def __init__(self, train_dataframe, eval_dataframe, label_to_id, model, train=True, caption_col='caption', label_col='label', img_col='image'):\n        self.train = train\n        if self.train:\n            self.data = train_dataframe\n            self.img_folder = IMAGE_TRAIN_FOLDER\n        else:\n            self.data = eval_dataframe\n            self.img_folder = IMAGE_TEST_FOLDER\n            \n        self.label_to_id = label_to_id\n        self.model = model\n        self.caption_col = caption_col\n        self.label_col = label_col\n        self.img_col = img_col\n        \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        row = self.data.iloc[index]\n        img_path = self.img_folder + row[self.img_col]\n        caption = row[self.caption_col]\n        if self.train:\n            label = self.label_to_id[row[self.label_col]]\n        else:\n            label = None\n#         # Ki·ªÉm tra n·∫øu m√¥ h√¨nh ƒë∆∞·ª£c b·ªçc trong DataParallel\n#         if isinstance(self.model, nn.DataParallel):\n#             preprocessor = self.model.module.preprocessor\n#             tokenizer = self.model.module.tokenizer\n#         else:\n#             preprocessor = self.model.preprocessor\n#             tokenizer = self.model.tokenizer\n        preprocessor = self.model.preprocessor\n        tokenizer = self.model.tokenizer\n\n        # Preprocess h√¨nh ·∫£nh v√† token h√≥a vƒÉn b·∫£n\n        image = preprocessor(Image.open(img_path))\n        text = tokenizer(caption)\n        \n        if self.train:\n            return image, text, label\n        else:\n            return image, text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torch.utils.data import DataLoader, WeightedRandomSampler\n\ndef create_oversampled_dataloader(dataset, label_col, batch_size, num_workers=4):\n    # ƒê·∫øm s·ªë l∆∞·ª£ng m·∫´u c·ªßa m·ªói class trong dataset ban ƒë·∫ßu\n    class_sample_counts = dataset.data[label_col].value_counts().to_dict()\n    \n    # T√≠nh tr·ªçng s·ªë cho m·ªói sample d·ª±a tr√™n s·ªë l∆∞·ª£ng m·∫´u c·ªßa class ƒë√≥\n    weights = 1. / np.array([class_sample_counts[label] for label in dataset.data[label_col]])\n    \n    # In th√¥ng tin v·ªÅ dataset tr∆∞·ªõc khi oversampling\n    print(\"S·ªë l∆∞·ª£ng m·∫´u c·ªßa m·ªói class tr∆∞·ªõc khi oversampling:\", class_sample_counts)\n\n    # T·∫°o WeightedRandomSampler v·ªõi tr·ªçng s·ªë ƒë√£ t√≠nh\n    sampler = WeightedRandomSampler(weights, num_samples=len(dataset), replacement=True)\n\n    # T·∫°o DataLoader v·ªõi sampler\n    dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler, num_workers=num_workers)\n\n    # T√≠nh alpha tr∆∞·ªõc khi oversampling\n    total_count_before = sum(class_sample_counts.values())\n    alpha_before_oversampling = {class_label: count / total_count_before for class_label, count in class_sample_counts.items()}\n    print(\"Alpha tr∆∞·ªõc khi oversampling:\", alpha_before_oversampling)\n\n    # T√≠nh s·ªë l∆∞·ª£ng m·∫´u sau khi oversampling\n    class_counts_after_sampling = {i: 0 for i in range(len(class_sample_counts))}\n    \n    for index in sampler:\n        label = dataset[index][2]  # Index 2 l√† nh√£n (label)\n        class_counts_after_sampling[label] += 1\n    \n    # In th√¥ng tin v·ªÅ dataset sau khi oversampling\n    print(\"S·ªë l∆∞·ª£ng m·∫´u c·ªßa m·ªói class sau khi oversampling:\", class_counts_after_sampling)\n\n    # T√≠nh alpha sau khi oversampling\n    total_count_after = sum(class_counts_after_sampling.values())\n    alpha_after_oversampling = {class_label: count / total_count_after for class_label, count in class_counts_after_sampling.items()}\n    print(\"Alpha sau khi oversampling:\", alpha_after_oversampling)\n\n    # Chuy·ªÉn alpha th√†nh tensor ƒë·ªÉ s·ª≠ d·ª•ng trong Focal Loss\n    alpha_for_focal = torch.tensor([alpha_after_oversampling[i] for i in range(len(alpha_after_oversampling))], dtype=torch.float32).to('cuda' if torch.cuda.is_available() else 'cpu')\n\n    return dataloader, alpha_for_focal\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# blank = test_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport open_clip\n\nclass MultimodalClassifier(nn.Module):\n    def __init__(self, model_name, pretrained, num_labels, device, mlp_hidden_size=512, dropout_prob=0.3):\n        super(MultimodalClassifier, self).__init__()\n        \n        # Load the pre-trained model and tokenizer\n        self.model, _, self.preprocessor = open_clip.create_model_and_transforms(model_name=model_name, pretrained=pretrained)\n        self.tokenizer = open_clip.get_tokenizer(model_name)\n        \n        # Fine-tuning the entire model\n        for param in self.model.parameters():\n            param.requires_grad = False\n        \n        # Extract the output dimension of the vision model\n        visual_output_dim = self.model.visual.trunk.patch_embed.proj.weight.shape[0]\n        #text_output_dim = self.model.transformer.width\n        # Define MLP layer with one hidden layer\n        self.mlp = nn.Sequential(\n            nn.Linear(2304, mlp_hidden_size),\n            nn.ReLU(),\n            nn.Dropout(dropout_prob)\n        )\n        #self.fusion_model = nn.Linear(2304,mlp_hidden_size)\n\n        # Classifier\n        self.classifier = nn.Linear(mlp_hidden_size, num_labels)\n        self.device = device\n        self.to(self.device)\n\n    def forward(self, images, texts):\n        # Ensure compatibility with DataParallel, if used\n        model = self.model.module if isinstance(self.model, nn.DataParallel) else self.model\n\n        # Extract image and text features\n        with torch.no_grad():\n            image_features = model.encode_image(images.to(self.device))\n        with torch.no_grad():\n            text_features = model.encode_text(texts.to(self.device))\n\n        # Concatenate features\n        combined_features = torch.cat((image_features, text_features), 1)\n        #combined_features = combined_features / combined_features.norm(dim=-1, keepdim=True)\n\n        # Pass through MLP\n        mlp_output = self.mlp(combined_features)\n\n        # Classify using the output of the MLP\n        logits = self.classifier(mlp_output)\n        return logits\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## training parameters to be used for all models ##\nnum_train_epochs = 30\nbatch_size = 256\nlearning_rate = 1e-2\nweight_decay = 0.001\nwarmup_steps = 2000","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.utils.data import DataLoader, WeightedRandomSampler, RandomSampler\nfrom transformers import get_scheduler\nfrom time import perf_counter\nfrom tqdm import tqdm, trange\nimport open_clip\nfrom pytorch_toolbelt import losses as L\n\n# Thi·∫øt l·∫≠p h·∫°t gi·ªëng ng·∫´u nhi√™n cho reproducibility\nset_seed(seed_val)\n\n# Load m√¥ h√¨nh OpenCLIP v√† th√™m classifier\nmodel_name = \"nllb-clip-large-siglip\"\npretrained = \"mrl\"\nnum_labels = 4  # multi-sarcasm, not-sarcasm, image-sarcasm\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nopenclip_model = MultimodalClassifier(model_name, pretrained, num_labels, device)\n\n# # N·∫øu c√≥ nhi·ªÅu h∆°n 1 GPU, √°p d·ª•ng DataParallel\n# device_ids = [0, 1]  # S·ª≠ d·ª•ng c·∫£ hai GPU\n# torch.cuda.set_device(device_ids[0])\n# openclip_model = nn.DataParallel(openclip_model, device_ids=device_ids)\n\n# if torch.cuda.device_count() > 1:\n#     print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n#     openclip_model = nn.DataParallel(openclip_model)\n\nopenclip_model = openclip_model.to(device)\n\n# T·∫°o dataset v√† dataloader\ntrain_dataset = MultimodalDataset(train_dataframe=df_train, eval_dataframe=df_test, label_to_id=label_to_id, model=openclip_model, train=True, caption_col='caption', label_col='label', img_col='image')\ntrain_dataloader, class_weights = create_oversampled_dataloader(train_dataset, label_col='label', batch_size=batch_size)  # TƒÉng num_workers ƒë·ªÉ t·∫£i d·ªØ li·ªáu song song\ntrain_sampler = RandomSampler(train_dataset)\n#train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=4)  # TƒÉng num_workers ƒë·ªÉ t·∫£i d·ªØ li·ªáu song song\n\n# S·ªë b∆∞·ªõc hu·∫•n luy·ªán t·ªïng\nt_total = len(train_dataloader) * num_train_epochs\n\n# Optimizer v√† Scheduler\noptimizer = AdamW(openclip_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\nscheduler = get_scheduler(name=\"cosine\", optimizer=optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n\nalpha = torch.tensor([1.78072665, 2.55869691, 24.44479595, 140.31891652])\nalpha = alpha.to(device)\n\n# Ti√™u chu·∫©n ƒë√°nh gi√° (loss function)\ncriterion = L.CrossEntropyFocalLoss(gamma=1.0, reduction='mean', class_weights=alpha)\n# criterion = torch.nn.CrossEntropyLoss()\n# T·∫°o scaler cho Mixed Precision Training\nscaler = torch.amp.GradScaler()\n\n# B·∫Øt ƒë·∫ßu qu√° tr√¨nh hu·∫•n luy·ªán\nopenclip_model.train()\n\nstart = perf_counter()\nfor epoch_num in trange(num_train_epochs, desc='Epochs'):\n    epoch_total_loss = 0\n\n    # Duy·ªát qua t·ª´ng batch trong train_dataloader\n    for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc='Batch'):\n        b_imgs, b_text, b_labels  = batch\n        \n        # Tokenize vƒÉn b·∫£n b·∫±ng OpenCLIP\n        b_inputs = b_text.squeeze(1).to(device)\n\n        # Chuy·ªÉn c√°c tensor d·ªØ li·ªáu sang thi·∫øt b·ªã ƒëang s·ª≠ d·ª•ng (GPU)\n        b_labels = b_labels.to(device)\n        b_imgs = b_imgs.to(device)\n\n        # Reset gradients\n        optimizer.zero_grad()\n\n        # S·ª≠ d·ª•ng mixed precision cho forward pass\n        with torch.amp.autocast('cuda'):\n            b_logits = openclip_model(b_imgs, b_inputs)\n            loss = criterion(b_logits, b_labels)\n        \n        # S·ª≠ d·ª•ng scaler ƒë·ªÉ scale gradients\n        scaler.scale(loss).backward()\n\n        # Gradient clipping ƒë·ªÉ tr√°nh exploding gradients\n        torch.nn.utils.clip_grad_norm_(openclip_model.parameters(), max_norm=1.0)\n\n        # C·∫≠p nh·∫≠t tr·ªçng s·ªë v·ªõi scaled gradients\n        scaler.step(optimizer)\n        scaler.update()\n        \n        # C·∫≠p nh·∫≠t scheduler\n        scheduler.step()\n\n        epoch_total_loss += loss.item()\n\n    # T√≠nh loss trung b√¨nh c·ªßa m·ªói epoch\n    avg_loss = epoch_total_loss / len(train_dataloader)\n\n    # In th√¥ng tin epoch\n    print(f'epoch = {epoch_num+1}')\n    print(f'    epoch_loss = {epoch_total_loss}')\n    print(f'    avg_epoch_loss = {avg_loss}')\n    print(f'    learning rate = {optimizer.param_groups[0][\"lr\"]}')\n    \n    torch.cuda.empty_cache()\n    \n# L∆∞u m√¥ h√¨nh sau m·ªói epoch\n#torch.save(openclip_model.state_dict(), 'openclip_model.pth')\n\n# T√≠nh th·ªùi gian hu·∫•n luy·ªán\nend = perf_counter()\ntraining_time = end - start\nprint(f'Training completed in {training_time} seconds')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(openclip_model.state_dict(), 'openclip.pth')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport json\nfrom torch.utils.data import DataLoader, SequentialSampler\nfrom tqdm import tqdm\n\n# Define device and initialize the model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nopenclip_model = openclip_model.to(device)\nopenclip_model.eval()\n\n# Initialize dataset and dataloader for testing\ntest_dataset = MultimodalDataset(train_dataframe=df_dev, eval_dataframe=df_test, label_to_id=label_to_id, model=openclip_model, train=True, caption_col='caption', label_col='label', img_col='image')\ntest_sampler = SequentialSampler(test_dataset)\ntest_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, sampler=test_sampler, num_workers=4)\n\n# Initialize results list\nprediction_results = []\n\n# Inference without gradient calculation\nwith torch.no_grad():\n    for step, batch in tqdm(enumerate(test_dataloader), total=len(test_dataloader), desc='Inference'):\n        b_imgs, b_text, b_labels = batch\n        \n        # Prepare inputs on the correct device\n        b_inputs = b_text.squeeze(1).to(device)\n        b_imgs = b_imgs.to(device)\n\n        # Forward pass\n        with torch.amp.autocast(device_type='cuda' if device.type == 'cuda' else 'cpu'):\n            logits = openclip_model(b_imgs, b_inputs)\n        \n        # Store the prediction results\n        prediction_results += torch.argmax(logits, dim=-1).tolist()\n\n# Map predicted IDs back to labels\nprediction_results = [id_to_label[p] for p in prediction_results]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_report = classification_report(df_dev['label'],prediction_results)\nclass_report","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#make prediciton\nimport torch\nimport json\nfrom torch.utils.data import DataLoader, SequentialSampler\nfrom tqdm import tqdm\n\n# ƒê∆∞·ªùng d·∫´n t·ªõi m√¥ h√¨nh ƒë√£ l∆∞u\n#model_path = '/kaggle/input/model-clip-moitrain/openclip_model.pth'  # C·∫≠p nh·∫≠t ƒë∆∞·ªùng d·∫´n t·ªõi m√¥ h√¨nh ƒë√£ l∆∞u\n\n# Load l·∫°i m√¥ h√¨nh OpenCLIP v√† th√™m classifier\n#model_name = \"nllb-clip-large-siglip\"\n#pretrained = \"mrl\"\nnum_labels = 4  # multi-sarcasm, not-sarcasm, image-sarcasm, text-sarcasm\n#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Kh·ªüi t·∫°o l·∫°i m√¥ h√¨nh MultimodalClassifier\n#openclip_model = MultimodalClassifier(model_name, pretrained, num_labels, device)\n\n# Load tr·ªçng s·ªë ƒë√£ l∆∞u v√†o m√¥ h√¨nh\n#openclip_model.load_state_dict(torch.load(model_path))\n\n# ƒê∆∞a m√¥ h√¨nh v·ªÅ ch·∫ø ƒë·ªô ƒë√°nh gi√° (eval mode)\n#openclip_model = openclip_model.to(device)\nopenclip_model.eval()\n\n# T·∫°o dataset v√† dataloader cho t·∫≠p test\ntest_dataset = MultimodalDataset(train_dataframe=df_train, eval_dataframe=df_test, label_to_id=label_to_id, model=openclip_model, train=False, caption_col='caption', label_col='label', img_col='image')\ntest_sampler = SequentialSampler(test_dataset)\ntest_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, sampler=test_sampler, num_workers=4)\n\n# Kh√¥ng t√≠nh gradient trong qu√° tr√¨nh inference\nresults = {}\nwith torch.no_grad():\n    for step, batch in tqdm(enumerate(test_dataloader), total=len(test_dataloader), desc='Inference'):\n        b_imgs, b_text = batch\n        \n        # Tokenize vƒÉn b·∫£n v√† chuy·ªÉn c√°c tensor d·ªØ li·ªáu sang thi·∫øt b·ªã ƒëang s·ª≠ d·ª•ng (GPU)\n        b_inputs = b_text.squeeze(1).to(device)\n        b_imgs = b_imgs.to(device)\n\n        # Th·ª±c hi·ªán forward pass\n        with torch.amp.autocast('cuda'):\n            logits = openclip_model(b_imgs, b_inputs)\n        \n        # L·∫•y nh√£n d·ª± ƒëo√°n (label d·ª± ƒëo√°n c√≥ x√°c su·∫•t l·ªõn nh·∫•t)\n        predictions = torch.argmax(logits, dim=1).cpu().numpy()\n\n        # L·∫•y id c·ªßa m·∫´u v√† l∆∞u nh√£n d·ª± ƒëo√°n\n        for idx, pred in zip(range(step * batch_size, (step + 1) * batch_size), predictions):\n            sample_id = test_dataset.data.iloc[idx].name  # Gi·∫£ s·ª≠ c·ªôt 'id' l√† index c·ªßa dataframe\n            label_name = {v: k for k, v in label_to_id.items()}[pred]  # ƒê·ªëi chi·∫øu nh√£n v·ªõi label_to_id\n            results[sample_id] = label_name\n\n# L∆∞u k·∫øt qu·∫£ v√†o file result.json\noutput = {\n    \"results\": results,\n    \"phase\": \"dev\"\n}\n\nwith open('results.json', 'w') as f:\n    json.dump(output, f, indent=4)\n\nprint(\"Inference completed. Results saved to results.json.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify the file path\nfile_path = \"/kaggle/working/training_report.txt\"\n\n# Save the report to a text file\nwith open(file_path, mode=\"w\") as file:\n    file.write(class_report)\n\nprint(f\"Report saved to {file_path}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc='Batch'):\n#         b_imgs, b_text, b_labels = batch\n#         print(b_imgs.shape)\n#         print(b_text.squeeze(1).shape)\n#         print(b_labels.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Gi·∫£ s·ª≠ b·∫°n ƒë√£ load m√¥ h√¨nh\n# import open_clip\n\n# # Load model t·ª´ open_clip\n# model_name = \"nllb-clip-large-siglip\"  # ho·∫∑c model b·∫°n ƒëang d√πng\n# pretrained = \"mrl\"  # T√™n c·ªßa m√¥ h√¨nh pretrained (n·∫øu c√≥)\n\n# # Load model v√† preprocess\n# model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms(model_name, pretrained=pretrained)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}