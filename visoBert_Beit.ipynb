{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["CAfBEaleFMqR"]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"904aa7e30dcb4d59b9467746b8c162d7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e959ac46311d445bbc1f378b19589f6b","IPY_MODEL_427879f091de4c6a81c7e2079584c7d6","IPY_MODEL_c1ca635e14a7454d8d8d12ca81200d9a"],"layout":"IPY_MODEL_487cb028d7504810a4438298c625bef3"}},"e959ac46311d445bbc1f378b19589f6b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9518d3fa2ced4eab980256400222b17d","placeholder":"‚Äã","style":"IPY_MODEL_651c70d8f698418694c709f966b0ee39","value":"config.json:‚Äá100%"}},"427879f091de4c6a81c7e2079584c7d6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_045d4096b3b34ffb8a689592d6848823","max":644,"min":0,"orientation":"horizontal","style":"IPY_MODEL_091a1a6434874be28374b1a35e1433b7","value":644}},"c1ca635e14a7454d8d8d12ca81200d9a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_225602a85a5b43e2b069edca5b68d165","placeholder":"‚Äã","style":"IPY_MODEL_a7ca6c9d3cc74b679af13542c98efc0b","value":"‚Äá644/644‚Äá[00:00&lt;00:00,‚Äá14.0kB/s]"}},"487cb028d7504810a4438298c625bef3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9518d3fa2ced4eab980256400222b17d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"651c70d8f698418694c709f966b0ee39":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"045d4096b3b34ffb8a689592d6848823":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"091a1a6434874be28374b1a35e1433b7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"225602a85a5b43e2b069edca5b68d165":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7ca6c9d3cc74b679af13542c98efc0b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd3ec0a42f7c483b926b95cc5420515d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b7c5d4b456ed48229ea3a9f05b88d371","IPY_MODEL_9b4197c3cf2e4ad2ac8bfdd761d986c2","IPY_MODEL_0b814eaa3e524d89af7af9651ee0adf2"],"layout":"IPY_MODEL_fa3ab71f647a40e2997408d67ab63f04"}},"b7c5d4b456ed48229ea3a9f05b88d371":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad8f02f9604642aeb821eee018224cad","placeholder":"‚Äã","style":"IPY_MODEL_3b51264cd9cc4e24b95655c120005fc6","value":"pytorch_model.bin:‚Äá100%"}},"9b4197c3cf2e4ad2ac8bfdd761d986c2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_512f9660e20d4d62a0bdbb384ecb9113","max":390403641,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f0978b748c0542aebde85092d66cf7ae","value":390403641}},"0b814eaa3e524d89af7af9651ee0adf2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb8b6b8a577e4cf09a5ff31e5157e3a3","placeholder":"‚Äã","style":"IPY_MODEL_8255833febd246d5bf682f0c6838425d","value":"‚Äá390M/390M‚Äá[00:16&lt;00:00,‚Äá20.3MB/s]"}},"fa3ab71f647a40e2997408d67ab63f04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad8f02f9604642aeb821eee018224cad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b51264cd9cc4e24b95655c120005fc6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"512f9660e20d4d62a0bdbb384ecb9113":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0978b748c0542aebde85092d66cf7ae":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fb8b6b8a577e4cf09a5ff31e5157e3a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8255833febd246d5bf682f0c6838425d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"861ad6074ae24df5a1eb3b375aa10e93":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c52eae0ff2224d0b84157336d457218c","IPY_MODEL_a2a3e1c647d1478baea2ad6e55e85f04","IPY_MODEL_205d17e4bc6b42d0ab6aae5c4845eb98"],"layout":"IPY_MODEL_6c995e0e1d7149ac94c857dad8bdc3e7"}},"c52eae0ff2224d0b84157336d457218c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_27e20c4ed00843eba71e97fcff8d3288","placeholder":"‚Äã","style":"IPY_MODEL_5313d00ba1eb43e38ff9af09905b6391","value":"sentencepiece.bpe.model:‚Äá100%"}},"a2a3e1c647d1478baea2ad6e55e85f04":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d17654c9c4044888a129f507cbc4d3f1","max":470732,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d96ab2ad49d2431c95ea6733f3866232","value":470732}},"205d17e4bc6b42d0ab6aae5c4845eb98":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5620429b5e644df5bb942e87c61c5183","placeholder":"‚Äã","style":"IPY_MODEL_c54e49c1ffd547f293e195e4f6dcf3ee","value":"‚Äá471k/471k‚Äá[00:00&lt;00:00,‚Äá27.0MB/s]"}},"6c995e0e1d7149ac94c857dad8bdc3e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27e20c4ed00843eba71e97fcff8d3288":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5313d00ba1eb43e38ff9af09905b6391":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d17654c9c4044888a129f507cbc4d3f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d96ab2ad49d2431c95ea6733f3866232":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5620429b5e644df5bb942e87c61c5183":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c54e49c1ffd547f293e195e4f6dcf3ee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9793698,"sourceType":"datasetVersion","datasetId":5856895},{"sourceId":9850807,"sourceType":"datasetVersion","datasetId":5879213}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport librosa\nimport pandas as pd","metadata":{"id":"hC7jAEClF1XP","execution":{"iopub.status.busy":"2024-11-08T15:17:35.419444Z","iopub.execute_input":"2024-11-08T15:17:35.420092Z","iopub.status.idle":"2024-11-08T15:17:40.498116Z","shell.execute_reply.started":"2024-11-08T15:17:35.420058Z","shell.execute_reply":"2024-11-08T15:17:40.497169Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Another version\n","metadata":{"id":"ctnXhJ28FKVJ"}},{"cell_type":"code","source":"import json\nimport os\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom transformers import AutoModel, AutoTokenizer, get_scheduler ,AutoModelForMaskedLM\nimport re\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom torch.optim import AdamW,Adam\nfrom tqdm.notebook import tqdm, trange\nfrom time import perf_counter\nfrom PIL import Image\nimport pandas as pd","metadata":{"id":"C08RsK2WIuj3","execution":{"iopub.status.busy":"2024-11-08T15:17:40.500079Z","iopub.execute_input":"2024-11-08T15:17:40.500630Z","iopub.status.idle":"2024-11-08T15:17:42.551885Z","shell.execute_reply.started":"2024-11-08T15:17:40.500586Z","shell.execute_reply":"2024-11-08T15:17:42.551119Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# set random seeds for repeatability\nimport numpy as np\nimport random\n\ndef set_seed(seed_val):\n    random.seed(seed_val)\n    np.random.seed(seed_val)\n    torch.manual_seed(seed_val)\n    torch.cuda.manual_seed_all(seed_val)\nseed_val = 0\nset_seed(seed_val)","metadata":{"id":"qxOdLLhTIxmI","execution":{"iopub.status.busy":"2024-11-08T15:17:42.552870Z","iopub.execute_input":"2024-11-08T15:17:42.553315Z","iopub.status.idle":"2024-11-08T15:17:42.564474Z","shell.execute_reply.started":"2024-11-08T15:17:42.553281Z","shell.execute_reply":"2024-11-08T15:17:42.563610Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndf=pd.read_csv('/kaggle/input/dsc-round1/train_ocr.csv')\nlen(df)","metadata":{"id":"NXlRN68ApfW-","execution":{"iopub.status.busy":"2024-11-08T15:17:42.566680Z","iopub.execute_input":"2024-11-08T15:17:42.566954Z","iopub.status.idle":"2024-11-08T15:17:42.787623Z","shell.execute_reply.started":"2024-11-08T15:17:42.566924Z","shell.execute_reply":"2024-11-08T15:17:42.786645Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"10805"},"metadata":{}}]},{"cell_type":"code","source":"import json\nimport pandas as pd\nwith open('/kaggle/input/dsc2024/vimmsd-private-test.json', 'r') as f:\n    data = json.load(f)\n\n# S·ª≠ d·ª•ng json_normalize ƒë·ªÉ l√†m ph·∫≥ng d·ªØ li·ªáu trong \"root\"\ndf_test = pd.DataFrame.from_dict(data, orient='index')\ndf_test.sample()","metadata":{"execution":{"iopub.status.busy":"2024-11-10T00:40:40.165106Z","iopub.execute_input":"2024-11-10T00:40:40.165970Z","iopub.status.idle":"2024-11-10T00:40:40.209718Z","shell.execute_reply.started":"2024-11-10T00:40:40.165913Z","shell.execute_reply":"2024-11-10T00:40:40.208780Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                                  image  \\\n1445  4d0a9e0270200b2fd565c3a10446f08f96149d87ac901b...   \n\n                                                caption label  \n1445  Thi·∫øu t.i·ªÅn c≈©ng ƒë∆∞·ª£c, ƒë·ª´ng thi·∫øu l√≤ng t·ª± tr·ªçn...  None  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>caption</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1445</th>\n      <td>4d0a9e0270200b2fd565c3a10446f08f96149d87ac901b...</td>\n      <td>Thi·∫øu t.i·ªÅn c≈©ng ƒë∆∞·ª£c, ƒë·ª´ng thi·∫øu l√≤ng t·ª± tr·ªçn...</td>\n      <td>None</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-11-10T00:41:35.058905Z","iopub.execute_input":"2024-11-10T00:41:35.059275Z","iopub.status.idle":"2024-11-10T00:41:35.064725Z","shell.execute_reply.started":"2024-11-10T00:41:35.059238Z","shell.execute_reply":"2024-11-10T00:41:35.063717Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"0\n","output_type":"stream"}]},{"cell_type":"code","source":"df['label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:17:42.826114Z","iopub.execute_input":"2024-11-08T15:17:42.826419Z","iopub.status.idle":"2024-11-08T15:17:42.840976Z","shell.execute_reply.started":"2024-11-08T15:17:42.826387Z","shell.execute_reply":"2024-11-08T15:17:42.840071Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"label\nnot-sarcasm      6062\nmulti-sarcasm    4224\nimage-sarcasm     442\ntext-sarcasm       77\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Create a copy of df for split_1\nsplit_1 = df.copy()\n\n# Create the 'not-sarcasm' column in the copy\n# Modify the 'label' column in split_1\nsplit_1['label'] = split_1['label'].apply(lambda x: 'not-sarcasm' if x == 'not-sarcasm' else 'sarcasm')\n\n# Display the first few rows to verify\nsplit_1.sample(10)\n# Display the first few rows to verify\nlen(split_1)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:17:42.842130Z","iopub.execute_input":"2024-11-08T15:17:42.842470Z","iopub.status.idle":"2024-11-08T15:17:42.859136Z","shell.execute_reply.started":"2024-11-08T15:17:42.842434Z","shell.execute_reply":"2024-11-08T15:17:42.858419Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"10805"},"metadata":{}}]},{"cell_type":"code","source":"# Filter the DataFrame for the specified labels and create a copy\nsplit_2 = df[df['label'].isin(['image-sarcasm', 'text-sarcasm', 'multi-sarcasm'])].copy()\n\n# Create a new column 'multi-sarcasm' in split_2\nsplit_2['label'] = split_2['label'].apply(lambda x: 'multi-sarcasm' if x == 'multi-sarcasm' else 'not-multi-sarcasm')\n\n# Display the first few rows to verify the new column\nsplit_2.sample(5)\nlen(split_2)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:17:42.860052Z","iopub.execute_input":"2024-11-08T15:17:42.860340Z","iopub.status.idle":"2024-11-08T15:17:42.875196Z","shell.execute_reply.started":"2024-11-08T15:17:42.860309Z","shell.execute_reply":"2024-11-08T15:17:42.874373Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"4743"},"metadata":{}}]},{"cell_type":"code","source":"# Filter the DataFrame for 'image-sarcasm' and 'text-sarcasm' and create a copy\nsplit_3 = df[df['label'].isin(['image-sarcasm', 'text-sarcasm'])].copy()\n\n# Encode 'image-sarcasm' as 1 and 'text-sarcasm' as 0\nsplit_3['label'] = split_3['label'].apply(lambda x: 'image-sarcasm' if x == 'image-sarcasm' else 'not-image-sarcasm')\n\n# Display the first few rows to verify\n(split_3.sample(5))\nlen(split_3)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:17:42.876258Z","iopub.execute_input":"2024-11-08T15:17:42.876531Z","iopub.status.idle":"2024-11-08T15:17:42.886833Z","shell.execute_reply.started":"2024-11-08T15:17:42.876501Z","shell.execute_reply":"2024-11-08T15:17:42.885865Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"519"},"metadata":{}}]},{"cell_type":"code","source":"type = 'gfdgfdg'\n# Image processing condition\nif type == 'not':\n    df_split = split_1.copy()\nelif type == 'multi':\n    df_split = split_2.copy()\nelif type == 'image':\n    df_split = split_3.copy()\nelse:\n    df_split = df.copy()\n\n# Fill NaNs with 'sarcasm'\ndf_split = df_split.applymap(lambda x: 'sarcasm' if x == ' ' else x)\n\n# Display a random sample of 20 rows\ndf_split.sample(10)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:17:42.892497Z","iopub.execute_input":"2024-11-08T15:17:42.892782Z","iopub.status.idle":"2024-11-08T15:17:42.949507Z","shell.execute_reply.started":"2024-11-08T15:17:42.892752Z","shell.execute_reply":"2024-11-08T15:17:42.948514Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/548335595.py:13: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  df_split = df_split.applymap(lambda x: 'sarcasm' if x == ' ' else x)\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"      Unnamed: 0  _key                                              image  \\\n9424        9424  9424  b93ba0882a054668e2f41d23bc5fe08c220c01571edd2c...   \n1560        1560  1560  6a0c5567b403058457571021b1140a8f8395298fc3b679...   \n9190        9190  9190  7f48657379fec679fd331f673639267455e5b104ed4c6b...   \n6775        6775  6775  2a50365927daa8d28c0d4f8d83ad500918f286a839a6fd...   \n2808        2808  2808  a91f0b7c22a49dd791417e69bb724f3186d656f5a37c39...   \n7031        7031  7031  8d9536dbc168bd90073171102f3a22377c254b0f773693...   \n2723        2723  2723  00431202d34cca500b6fa9f7976fc3bd9f6bb98d525deb...   \n4714        4714  4714  46827d1caad00dc07b80d1863c969d10f2da82620e1a61...   \n3041        3041  3041  5727101c2f08ff96e02f1de2cc92cdb1d5459c530a15ad...   \n1779        1779  1779  c90b35765c434dfefddac499b96999a4ccfe92aa501bbf...   \n\n                                                caption          label  \\\n9424                           L√†m g√¨ c≈©ng ph·∫£i c√≥ VƒÇN!  multi-sarcasm   \n1560  T·∫°m qu√™n ƒëi s·ª± th·∫•t v·ªçng v√† ti·∫øc nu·ªëi sau tr·∫≠n...    not-sarcasm   \n9190  Xiumin (EXO) ƒë√£ h·∫° c√°nh t·∫°i S√¢n bay n·ªôi b√†i tr...    not-sarcasm   \n6775  THU·ª≤ TI√äN ƒê·∫∏P KH√îNG G√ìC CH·∫æT B√äN C·∫†NH YAMAHA G...    not-sarcasm   \n2808                                             H·ª£p l√Ω  multi-sarcasm   \n7031  H√Ä N·ªòI S·∫ÆP C√ì SHOW DI·ªÑN TH·ª∞C C·∫¢NH TR√äN S√îNG VE...    not-sarcasm   \n2723                                   √Åp l·ª±c c√¥ng vi·ªác  multi-sarcasm   \n4714  Trong c·∫∑p th·∫±ng n√†o c√≥ 10 t·ªù n√†y th√¨ gi·ªù ki·ªÉm ...  multi-sarcasm   \n3041                   T√¢y Du K√Ω phi√™n b·∫£n s·∫Øc n√©t 4K üòå    not-sarcasm   \n1779  S√°ng nay, 6/6 h∆°n 96.000 h·ªçc sinh t·∫°i TPHCM ch...    not-sarcasm   \n\n                                                   ocr1  \n9424  \"N√≥ h√†nh tao t·ªën ti·ªÅn qu√°n\", \"b√°n m·∫π ƒëi cho kh...  \n1560  Anh Vria, 16, ƒêTU22 Vi·ªát Nam, th√¨ t√≠ch c·ª±c t·∫≠p...  \n9190                                                 09  \n6775                                            sarcasm  \n2808  L√†m th·∫ø n√†o ƒë·ªÉ h√¥ bi·∫øn m·ª©c l∆∞∆°ng 5trung, 1 th√†...  \n7031                                            sarcasm  \n2723  Anh xem giup em, Xem g√¨ em, C√≥ ph·∫£i em b√©o l√™n...  \n4714  Tr∆∞·ªùng :, Th·ª©, ng√†y, th√°ng, nƒÉm, L·ªõp:, KI·ªÇM TR...  \n3041                                            sarcasm  \n1779                                            sarcasm  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>_key</th>\n      <th>image</th>\n      <th>caption</th>\n      <th>label</th>\n      <th>ocr1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>9424</th>\n      <td>9424</td>\n      <td>9424</td>\n      <td>b93ba0882a054668e2f41d23bc5fe08c220c01571edd2c...</td>\n      <td>L√†m g√¨ c≈©ng ph·∫£i c√≥ VƒÇN!</td>\n      <td>multi-sarcasm</td>\n      <td>\"N√≥ h√†nh tao t·ªën ti·ªÅn qu√°n\", \"b√°n m·∫π ƒëi cho kh...</td>\n    </tr>\n    <tr>\n      <th>1560</th>\n      <td>1560</td>\n      <td>1560</td>\n      <td>6a0c5567b403058457571021b1140a8f8395298fc3b679...</td>\n      <td>T·∫°m qu√™n ƒëi s·ª± th·∫•t v·ªçng v√† ti·∫øc nu·ªëi sau tr·∫≠n...</td>\n      <td>not-sarcasm</td>\n      <td>Anh Vria, 16, ƒêTU22 Vi·ªát Nam, th√¨ t√≠ch c·ª±c t·∫≠p...</td>\n    </tr>\n    <tr>\n      <th>9190</th>\n      <td>9190</td>\n      <td>9190</td>\n      <td>7f48657379fec679fd331f673639267455e5b104ed4c6b...</td>\n      <td>Xiumin (EXO) ƒë√£ h·∫° c√°nh t·∫°i S√¢n bay n·ªôi b√†i tr...</td>\n      <td>not-sarcasm</td>\n      <td>09</td>\n    </tr>\n    <tr>\n      <th>6775</th>\n      <td>6775</td>\n      <td>6775</td>\n      <td>2a50365927daa8d28c0d4f8d83ad500918f286a839a6fd...</td>\n      <td>THU·ª≤ TI√äN ƒê·∫∏P KH√îNG G√ìC CH·∫æT B√äN C·∫†NH YAMAHA G...</td>\n      <td>not-sarcasm</td>\n      <td>sarcasm</td>\n    </tr>\n    <tr>\n      <th>2808</th>\n      <td>2808</td>\n      <td>2808</td>\n      <td>a91f0b7c22a49dd791417e69bb724f3186d656f5a37c39...</td>\n      <td>H·ª£p l√Ω</td>\n      <td>multi-sarcasm</td>\n      <td>L√†m th·∫ø n√†o ƒë·ªÉ h√¥ bi·∫øn m·ª©c l∆∞∆°ng 5trung, 1 th√†...</td>\n    </tr>\n    <tr>\n      <th>7031</th>\n      <td>7031</td>\n      <td>7031</td>\n      <td>8d9536dbc168bd90073171102f3a22377c254b0f773693...</td>\n      <td>H√Ä N·ªòI S·∫ÆP C√ì SHOW DI·ªÑN TH·ª∞C C·∫¢NH TR√äN S√îNG VE...</td>\n      <td>not-sarcasm</td>\n      <td>sarcasm</td>\n    </tr>\n    <tr>\n      <th>2723</th>\n      <td>2723</td>\n      <td>2723</td>\n      <td>00431202d34cca500b6fa9f7976fc3bd9f6bb98d525deb...</td>\n      <td>√Åp l·ª±c c√¥ng vi·ªác</td>\n      <td>multi-sarcasm</td>\n      <td>Anh xem giup em, Xem g√¨ em, C√≥ ph·∫£i em b√©o l√™n...</td>\n    </tr>\n    <tr>\n      <th>4714</th>\n      <td>4714</td>\n      <td>4714</td>\n      <td>46827d1caad00dc07b80d1863c969d10f2da82620e1a61...</td>\n      <td>Trong c·∫∑p th·∫±ng n√†o c√≥ 10 t·ªù n√†y th√¨ gi·ªù ki·ªÉm ...</td>\n      <td>multi-sarcasm</td>\n      <td>Tr∆∞·ªùng :, Th·ª©, ng√†y, th√°ng, nƒÉm, L·ªõp:, KI·ªÇM TR...</td>\n    </tr>\n    <tr>\n      <th>3041</th>\n      <td>3041</td>\n      <td>3041</td>\n      <td>5727101c2f08ff96e02f1de2cc92cdb1d5459c530a15ad...</td>\n      <td>T√¢y Du K√Ω phi√™n b·∫£n s·∫Øc n√©t 4K üòå</td>\n      <td>not-sarcasm</td>\n      <td>sarcasm</td>\n    </tr>\n    <tr>\n      <th>1779</th>\n      <td>1779</td>\n      <td>1779</td>\n      <td>c90b35765c434dfefddac499b96999a4ccfe92aa501bbf...</td>\n      <td>S√°ng nay, 6/6 h∆°n 96.000 h·ªçc sinh t·∫°i TPHCM ch...</td>\n      <td>not-sarcasm</td>\n      <td>sarcasm</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def text_to_emoji(text):\n    emoji_map = {\n        \":))\": \"üòä\",\n        \"=)))\": \"üòä\",# Happy\n        \":((\": \"üòû\",  # Sad\n        \":'(\": \"üò¢\",  # Crying\n        \":D\": \"üòÉ\",   # Big smile\n        \":(\": \"‚òπÔ∏è\",   # Disappointed\n        \":|\": \"üòê\",   # Neutral\n    }\n    for text_emoji, real_emoji in emoji_map.items():\n        text = text.replace(text_emoji, real_emoji)\n    return text\n\n# Preprocessing function\ndef preprocess_text(text):\n    text = text.lower()\n    text = text_to_emoji(text)\n    text = re.sub(r'[^\\w\\s,]', '', text)\n    text = re.sub(r'\\d+', '', text)\n    text = ' '.join(text.split())\n    return text\n\ndf_split['caption']=df_split['caption'].apply(preprocess_text).apply(text_to_emoji)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:17:42.950952Z","iopub.execute_input":"2024-11-08T15:17:42.951330Z","iopub.status.idle":"2024-11-08T15:17:43.357660Z","shell.execute_reply.started":"2024-11-08T15:17:42.951288Z","shell.execute_reply":"2024-11-08T15:17:43.356878Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df_split['caption']","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:17:43.358644Z","iopub.execute_input":"2024-11-08T15:17:43.358922Z","iopub.status.idle":"2024-11-08T15:17:43.366555Z","shell.execute_reply.started":"2024-11-08T15:17:43.358891Z","shell.execute_reply":"2024-11-08T15:17:43.365511Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"0                       c√¥ ·∫•y tr√™n m·∫°ng vs c√¥ ·∫•y ngo√†i ƒë·ªùi\n1               ng∆∞·ªùi t√¢m linh giao ti·∫øp v·ªõi ng∆∞·ªùi th·ª±c t·∫ø\n2        h√¨nh nh∆∞ trƒÉng h√¥m nay ƒë·∫πp qu√° m·ªçi ng∆∞·ªùi ·∫° canhco\n3        m·ªçi ng∆∞·ªùi nghƒ© sao v·ªÅ ph√°t bi·ªÉu c·ªßa shark vi·ªát...\n4                tay hai n√†ng ch·ª© vi·ªác g√¨ ph·∫£i l·ªá hai h√†ng\n                               ...                        \n10800                                          l·ªôn ƒë·∫ßu r·ªìi\n10801    ch√†o c√°c b·∫°n, m√¨nh l√† goda takeshi trong live ...\n10802                                   cre h√πynh qu·ªëc huy\n10803                                     anh h√πng th·∫≠t s·ª±\n10804                  qu√° l√† b√¨nh th∆∞·ªùng butchr interpool\nName: caption, Length: 10805, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# split train and dev\ndf_train, df_dev= train_test_split(df_split, test_size=0.1, random_state=42)","metadata":{"id":"hAl91U8bFutY","execution":{"iopub.status.busy":"2024-11-08T15:17:43.367869Z","iopub.execute_input":"2024-11-08T15:17:43.368186Z","iopub.status.idle":"2024-11-08T15:17:43.386437Z","shell.execute_reply.started":"2024-11-08T15:17:43.368144Z","shell.execute_reply":"2024-11-08T15:17:43.385747Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, WeightedRandomSampler\nimport numpy as np\n\n# Assuming 'df' is your DataFrame and 'label' is the column with class labels\n\n# Step 1: Calculate the sampling weight for each class\nclass_counts = df_train['label'].value_counts()\nclass_weights = 1.0 / class_counts\nsample_weights = df_train['label'].map(class_weights).values  # Map the class weights to each sample","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:17:43.387363Z","iopub.execute_input":"2024-11-08T15:17:43.387609Z","iopub.status.idle":"2024-11-08T15:17:43.395734Z","shell.execute_reply.started":"2024-11-08T15:17:43.387582Z","shell.execute_reply":"2024-11-08T15:17:43.394783Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(len(df_train))\nprint(len(df_dev))\nprint(len(df_test))","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:17:43.396896Z","iopub.execute_input":"2024-11-08T15:17:43.397267Z","iopub.status.idle":"2024-11-08T15:17:43.406673Z","shell.execute_reply.started":"2024-11-08T15:17:43.397231Z","shell.execute_reply":"2024-11-08T15:17:43.405866Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"9724\n1081\n1413\n","output_type":"stream"}]},{"cell_type":"code","source":"IMAGE_TRAIN_FOLDER='/kaggle/input/dsc2024/training-images/train-images/'\nIMAGE_TEST_FOLDER='/kaggle/input/dsc2024/private-test-images/test-images/'","metadata":{"id":"DXNL5B4iFw8c","execution":{"iopub.status.busy":"2024-11-08T15:17:43.407830Z","iopub.execute_input":"2024-11-08T15:17:43.408155Z","iopub.status.idle":"2024-11-08T15:17:43.412774Z","shell.execute_reply.started":"2024-11-08T15:17:43.408124Z","shell.execute_reply":"2024-11-08T15:17:43.411923Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#Encode labels\nlabel_to_id = {lab:i for i, lab in enumerate(df_train['label'].sort_values().unique())}\nid_to_label = {v:k for k,v in label_to_id.items()}\nlabel_to_id","metadata":{"id":"HcKn4aXoF3wo","outputId":"c955d042-988a-4b37-e598-3f12f8f2164c","execution":{"iopub.status.busy":"2024-11-08T15:17:43.413840Z","iopub.execute_input":"2024-11-08T15:17:43.414738Z","iopub.status.idle":"2024-11-08T15:17:43.433132Z","shell.execute_reply.started":"2024-11-08T15:17:43.414706Z","shell.execute_reply":"2024-11-08T15:17:43.432390Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"{'image-sarcasm': 0, 'multi-sarcasm': 1, 'not-sarcasm': 2, 'text-sarcasm': 3}"},"metadata":{}}]},{"cell_type":"code","source":"num_out_labels = len(label_to_id)\nprint(\"Number of labels \", num_out_labels)","metadata":{"id":"bfKHoEsfIQkh","outputId":"446eb7d2-6328-4b43-9513-81aac38dd376","execution":{"iopub.status.busy":"2024-11-08T15:17:43.434121Z","iopub.execute_input":"2024-11-08T15:17:43.434436Z","iopub.status.idle":"2024-11-08T15:17:43.438696Z","shell.execute_reply.started":"2024-11-08T15:17:43.434404Z","shell.execute_reply":"2024-11-08T15:17:43.437794Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Number of labels  4\n","output_type":"stream"}]},{"cell_type":"code","source":"# extract layers of resnet-50 to build a new model\n\nimport torch.nn as nn\nfrom torchvision.models.resnet import resnet50\n\nclass ResNetFeatureModel(nn.Module):\n    def __init__(self, output_layer):\n        super().__init__()\n        self.output_layer = output_layer \n        pretrained_resnet = resnet50(pretrained=True)\n        self.children_list = []\n        for n,c in pretrained_resnet.named_children():\n            self.children_list.append(c)\n            if n == self.output_layer:\n                break\n\n        self.net = nn.Sequential(*self.children_list)\n\n\n    def forward(self,x):\n        x = self.net(x)\n        x = torch.flatten(x, 1)\n        return x","metadata":{"id":"jZS6yB6dFXQC","execution":{"iopub.status.busy":"2024-11-08T15:17:43.439918Z","iopub.execute_input":"2024-11-08T15:17:43.440716Z","iopub.status.idle":"2024-11-08T15:17:43.447893Z","shell.execute_reply.started":"2024-11-08T15:17:43.440666Z","shell.execute_reply":"2024-11-08T15:17:43.446976Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import BeitModel, BeitConfig\n\nclass BeitFeatureModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Load pre-trained BEiT model\n        self.beit = BeitModel.from_pretrained(\"microsoft/beit-base-patch16-224\")\n\n    def forward(self, x):\n        # Extract features for the image\n        outputs = self.beit(pixel_values=x)\n        # Get the [CLS] token representation\n        img_features = outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, hidden_size)\n        return img_features\n","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:17:43.449004Z","iopub.execute_input":"2024-11-08T15:17:43.449365Z","iopub.status.idle":"2024-11-08T15:17:43.495767Z","shell.execute_reply.started":"2024-11-08T15:17:43.449324Z","shell.execute_reply":"2024-11-08T15:17:43.495090Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom PIL import Image\nfrom torchvision import transforms\n\nclass MultimodalDataset(Dataset):\n    def __init__(self, df, label_to_id=None, mode='train', text_field=\"caption\", label_field='label', image_path_field=\"image\"):\n        \"\"\"\n        Args:\n            df (DataFrame): The DataFrame containing your data.\n            label_to_id (dict): Dictionary for mapping labels to IDs. Set to None for test data.\n            mode (str): Mode of the dataset. One of ['train', 'test', 'dev'].\n            text_field (str): Column name for text data.\n            label_field (str): Column name for label data.\n            image_path_field (str): Column name for image paths.\n        \"\"\"\n        self.df = df.reset_index(drop=True)\n        self.label_to_id = label_to_id\n        self.mode = mode  # Mode can be 'train', 'test', or 'dev'\n        self.text_field = text_field\n        #self.ocr=ocr_field\n        self.label_field = label_field\n        self.image_path_field = image_path_field\n\n        # ResNet-50 settings\n        self.img_size = 224\n        self.mean, self.std = (\n            0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)\n\n        # Define different transformations based on the mode\n        self.train_transform_func = transforms.Compose([\n            transforms.RandomResizedCrop(self.img_size, scale=(0.5, 1.0)),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize(self.mean, self.std)\n        ])\n\n        self.test_transform_func = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(self.img_size),\n            transforms.ToTensor(),\n            transforms.Normalize(self.mean, self.std)\n        ])\n\n        self.dev_transform_func = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(self.img_size),\n            transforms.ToTensor(),\n            transforms.Normalize(self.mean, self.std)\n        ])\n\n    def __getitem__(self, index):\n        # Get text data\n        text = str(self.df.at[index, self.text_field])\n        \n        # Select the correct image folder based on mode\n        if self.mode == 'test':\n            img_path = IMAGE_TEST_FOLDER + self.df.at[index, self.image_path_field]\n        else:\n            img_path = IMAGE_TRAIN_FOLDER + self.df.at[index, self.image_path_field]\n\n        # Load the image\n        image = Image.open(img_path).convert('RGB')  # Ensure the image is in RGB format\n\n        # Apply appropriate transformations based on mode\n        if self.mode == 'train':\n            img = self.train_transform_func(image)\n        elif self.mode == 'test':\n            img = self.test_transform_func(image)\n        elif self.mode == 'dev':\n            img = self.dev_transform_func(image)\n\n        # If labels are available, return them, else only return the image and text\n        if self.label_to_id is not None and self.label_field in self.df.columns:\n            label = self.label_to_id[self.df.at[index, self.label_field]]\n            return text, label, img\n        else:\n            return text, img\n\n    def __len__(self):\n        return self.df.shape[0]\n","metadata":{"id":"gqcCN29AFL-t","execution":{"iopub.status.busy":"2024-11-08T15:17:43.496947Z","iopub.execute_input":"2024-11-08T15:17:43.497249Z","iopub.status.idle":"2024-11-08T15:17:43.512657Z","shell.execute_reply.started":"2024-11-08T15:17:43.497191Z","shell.execute_reply":"2024-11-08T15:17:43.511757Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"class VisoBertBeitModel(nn.Module):\n    def __init__(self, num_labels, text_pretrained='uitnlp/visobert', mlp_hidden_size=512, dropout_prob=0.3):\n        super().__init__()\n        # Text encoder (ViSoBERT)\n        self.text_encoder = AutoModel.from_pretrained(text_pretrained)\n        self.tokenizer = AutoTokenizer.from_pretrained(text_pretrained)\n\n        # Visual encoder (BEiT)\n        self.visual_encoder = BeitFeatureModel()\n        \n        # Check hidden sizes\n        self.text_hidden_size = self.text_encoder.config.hidden_size\n        self.image_hidden_size = self.visual_encoder.beit.config.hidden_size\n\n        # MLP with one hidden layer\n        self.mlp = nn.Sequential(\n            nn.Linear(self.text_hidden_size + self.image_hidden_size, mlp_hidden_size),\n            nn.ReLU(),\n            nn.Dropout(dropout_prob),\n        )\n\n        # Classifier layer\n        self.classifier = nn.Linear(mlp_hidden_size, num_labels)\n\n    def forward(self, text, image):\n        # Encode text and extract the [CLS] token feature\n        text_output = self.text_encoder(**text)\n        text_feature = text_output.last_hidden_state[:, 0, :]  # Shape: (batch_size, text_hidden_size)\n\n        # Encode image features\n        img_feature = self.visual_encoder(image)  # Shape: (batch_size, image_hidden_size)\n        \n        # Concatenate text and image features\n        features = torch.cat((text_feature, img_feature), dim=1)  # Shape: (batch_size, text_hidden_size + image_hidden_size)\n\n        # Pass through the MLP layer\n        mlp_output = self.mlp(features)\n\n        # Classify using the final output of the MLP\n        logits = self.classifier(mlp_output)\n\n        return logits","metadata":{"id":"xZHp2kC5FZ6v","execution":{"iopub.status.busy":"2024-11-08T15:17:43.514075Z","iopub.execute_input":"2024-11-08T15:17:43.514407Z","iopub.status.idle":"2024-11-08T15:17:43.524804Z","shell.execute_reply.started":"2024-11-08T15:17:43.514376Z","shell.execute_reply":"2024-11-08T15:17:43.524034Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torch.utils.data import DataLoader, WeightedRandomSampler\n\ndef create_oversampled_dataloader(dataset, label_col, batch_size, num_workers=4):\n    # ƒê·∫øm s·ªë l∆∞·ª£ng m·∫´u c·ªßa m·ªói class trong dataset ban ƒë·∫ßu\n    class_sample_counts = dataset.df[label_col].value_counts().to_dict()\n    \n    # T√≠nh tr·ªçng s·ªë cho m·ªói sample d·ª±a tr√™n s·ªë l∆∞·ª£ng m·∫´u c·ªßa class ƒë√≥\n    weights = 1. / np.array([class_sample_counts[label] for label in dataset.df[label_col]])\n    \n    # In th√¥ng tin v·ªÅ dataset tr∆∞·ªõc khi oversampling\n    print(\"S·ªë l∆∞·ª£ng m·∫´u c·ªßa m·ªói class tr∆∞·ªõc khi oversampling:\", class_sample_counts)\n\n    # T·∫°o WeightedRandomSampler v·ªõi tr·ªçng s·ªë ƒë√£ t√≠nh\n    sampler = WeightedRandomSampler(weights, num_samples=len(dataset), replacement=True)\n\n    # T·∫°o DataLoader v·ªõi sampler\n    dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler, num_workers=num_workers)\n\n    # T√≠nh alpha tr∆∞·ªõc khi oversampling\n    total_count_before = sum(class_sample_counts.values())\n    alpha_before_oversampling = {class_label: count / total_count_before for class_label, count in class_sample_counts.items()}\n    print(\"Alpha tr∆∞·ªõc khi oversampling:\", alpha_before_oversampling)\n\n    # T√≠nh s·ªë l∆∞·ª£ng m·∫´u sau khi oversampling\n    class_counts_after_sampling = {i: 0 for i in range(len(class_sample_counts))}\n    \n    for index in sampler:\n        label = dataset[index][2]  # Index 2 l√† nh√£n (label)\n        class_counts_after_sampling[label] += 1\n    \n    # In th√¥ng tin v·ªÅ dataset sau khi oversampling\n    print(\"S·ªë l∆∞·ª£ng m·∫´u c·ªßa m·ªói class sau khi oversampling:\", class_counts_after_sampling)\n\n    # T√≠nh alpha sau khi oversampling\n    total_count_after = sum(class_counts_after_sampling.values())\n    alpha_after_oversampling = {class_label: count / total_count_after for class_label, count in class_counts_after_sampling.items()}\n    print(\"Alpha sau khi oversampling:\", alpha_after_oversampling)\n\n    # Chuy·ªÉn alpha th√†nh tensor ƒë·ªÉ s·ª≠ d·ª•ng trong Focal Loss\n    alpha_for_focal = torch.tensor([alpha_after_oversampling[i] for i in range(len(alpha_after_oversampling))], dtype=torch.float32).to('cuda' if torch.cuda.is_available() else 'cpu')\n\n    return dataloader, alpha_for_focal\n","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:17:43.526105Z","iopub.execute_input":"2024-11-08T15:17:43.526471Z","iopub.status.idle":"2024-11-08T15:17:43.540141Z","shell.execute_reply.started":"2024-11-08T15:17:43.526432Z","shell.execute_reply":"2024-11-08T15:17:43.539239Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"len(label_to_id)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:17:43.541295Z","iopub.execute_input":"2024-11-08T15:17:43.542067Z","iopub.status.idle":"2024-11-08T15:17:43.552634Z","shell.execute_reply.started":"2024-11-08T15:17:43.542034Z","shell.execute_reply":"2024-11-08T15:17:43.551761Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"4"},"metadata":{}}]},{"cell_type":"code","source":"import torch\n\n# Specify the path to the saved model\nmodel_path = \"/kaggle/input/dsc2024/resnet_model_v8.pth\"\n\n# Assuming the same model architecture is defined\nmodel = VisoBertBeitModel(num_labels=num_out_labels, text_pretrained='uitnlp/visobert')\n#model = VisoBertResNetModel(num_labels=len(label_to_id), text_pretrained='vinai/phobert-base')\n\n # Replace `MyModel` with your actual model class\n\n# Load the state_dict from the file\nmodel.load_state_dict(torch.load(model_path))\n\n# Move the model to the appropriate device (CPU or GPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel=model.to(device)\n\n# Set the model to evaluation mode\n#model.train()\n\n#print(f\"Model loaded from {model_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:17:43.553722Z","iopub.execute_input":"2024-11-08T15:17:43.554000Z","iopub.status.idle":"2024-11-08T15:17:58.003758Z","shell.execute_reply.started":"2024-11-08T15:17:43.553970Z","shell.execute_reply":"2024-11-08T15:17:58.002260Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/644 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1ab28e0ce46494b9a95f6d2142d07cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/390M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"458b1f1d77814fc084736d4f0ce369c0"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaModel were not initialized from the model checkpoint at uitnlp/visobert and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/471k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff5e380a47c14fb5ae0b0a017b2c0743"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/69.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0d3c1694f7a422eab4bca219f58b702"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/350M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd7f920c108b470ebabb9b707dbe7459"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_30/849942651.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path))\n","output_type":"stream"}]},{"cell_type":"code","source":"len(df_train)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:17:58.007086Z","iopub.execute_input":"2024-11-08T15:17:58.008074Z","iopub.status.idle":"2024-11-08T15:17:58.014893Z","shell.execute_reply.started":"2024-11-08T15:17:58.008028Z","shell.execute_reply":"2024-11-08T15:17:58.013897Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"9724"},"metadata":{}}]},{"cell_type":"code","source":"!nvidia-smi -L","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:17:58.016076Z","iopub.execute_input":"2024-11-08T15:17:58.016454Z","iopub.status.idle":"2024-11-08T15:18:00.518823Z","shell.execute_reply.started":"2024-11-08T15:17:58.016415Z","shell.execute_reply":"2024-11-08T15:18:00.517625Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"GPU 0: Tesla T4 (UUID: GPU-2d2aefef-4c29-4559-6c8f-6794e2121533)\nGPU 1: Tesla T4 (UUID: GPU-7e685d48-1d93-5c8a-6320-84cb3f574da4)\n","output_type":"stream"}]},{"cell_type":"code","source":"# parameters\ntraining_params = {\n    \"seed_val\": seed_val,\n    \"training_size\" : len(df_train),\n    \"dev_size\": len(df_dev),\n    \"test_size\": len(df_test),\n    \"num_train_epochs\": 30,\n    \"batch_size\": 16,\n    \"learning_rate\": 1e-5, \n    \"weight_decay\": 0.01,\n    \"warmup_steps\": 0,\n    \"max_seq_length\": 512\n}\n","metadata":{"id":"WlAYOob2Fqqq","execution":{"iopub.status.busy":"2024-11-08T15:18:00.527266Z","iopub.execute_input":"2024-11-08T15:18:00.527671Z","iopub.status.idle":"2024-11-08T15:18:00.538288Z","shell.execute_reply.started":"2024-11-08T15:18:00.527626Z","shell.execute_reply":"2024-11-08T15:18:00.533226Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Accessing each value by key\nseed_val = training_params['seed_val']\ntraining_size = training_params['training_size']\ndev_size = training_params['dev_size']\ntest_size = training_params['test_size']\nnum_train_epochs = training_params['num_train_epochs']\nbatch_size = training_params['batch_size']\nlearning_rate = training_params['learning_rate']\nweight_decay = training_params['weight_decay']\nwarmup_steps = training_params['warmup_steps']\nmax_seq_length = training_params['max_seq_length']\n","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:18:00.539547Z","iopub.execute_input":"2024-11-08T15:18:00.539826Z","iopub.status.idle":"2024-11-08T15:18:01.538315Z","shell.execute_reply.started":"2024-11-08T15:18:00.539796Z","shell.execute_reply":"2024-11-08T15:18:01.537283Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"training_params","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:18:01.539624Z","iopub.execute_input":"2024-11-08T15:18:01.539937Z","iopub.status.idle":"2024-11-08T15:18:01.549959Z","shell.execute_reply.started":"2024-11-08T15:18:01.539905Z","shell.execute_reply":"2024-11-08T15:18:01.549176Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"{'seed_val': 0,\n 'training_size': 9724,\n 'dev_size': 1081,\n 'test_size': 1413,\n 'num_train_epochs': 10,\n 'batch_size': 32,\n 'learning_rate': 1e-05,\n 'weight_decay': 0.01,\n 'warmup_steps': 0,\n 'max_seq_length': 512}"},"metadata":{}}]},{"cell_type":"code","source":"\n\n# Step 2: Create the WeightedRandomSampler\n#sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:18:01.551930Z","iopub.execute_input":"2024-11-08T15:18:01.552737Z","iopub.status.idle":"2024-11-08T15:18:01.556773Z","shell.execute_reply.started":"2024-11-08T15:18:01.552694Z","shell.execute_reply":"2024-11-08T15:18:01.555828Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# training step\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.utils.data import DataLoader, RandomSampler\nfrom tqdm import trange, tqdm\nimport torch.nn as nn\nfrom transformers import get_scheduler, AdamW\nimport time\n\n\n# Set up gradient accumulation steps and use mixed precision\naccumulation_steps = 4  # Perform backward pass and optimizer step after this many batches\nscaler = GradScaler()   # For mixed precision\n\ntrain_dataset = MultimodalDataset(df=df_train, label_to_id=label_to_id, mode='train', text_field='caption', label_field='label', image_path_field='image')\ntrain_sampler = RandomSampler(train_dataset)\ntrain_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, sampler=train_sampler)\n#train_dataloader, class_weights = create_oversampled_dataloader(train_dataset, label_col='label', batch_size=batch_size)  # TƒÉng num_workers ƒë·ªÉ t·∫£i d·ªØ li·ªáu song song\n\n# Step 3: Use the sampler in the DataLoader\n#train_dataloader = DataLoader(\n #   df_train,        # Replace with your dataset object\n  #  batch_size=batch_size,       # Set batch size as desired\n   # sampler=sampler      # Pass the sampler\n#)\nt_total = len(train_dataloader) * num_train_epochs\n\noptimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\nscheduler = get_scheduler(name=\"cosine\", optimizer=optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n\n# Set the device to the second GPU (GPU 1)\n# Move model to the device\ncriterion = nn.CrossEntropyLoss()\n#criterion = L.CrossEntropyFocalLoss(gamma=1.0, reduction='mean', class_weights=class_weights)\n\nstart = time.perf_counter()\n\n#for param in model.text_encoder.parameters():\n #   param.requires_grad = True\n\n#for param in model.visual_encoder.parameters():\n #   param.requires_grad = True\n\n\n# Initialize lists to store epoch loss and learning rate\nepoch_losses = []\nlearning_rates = []\n\n# Set up training loop with tqdm only for epochs\nfor epoch_num in trange(num_train_epochs, desc='Epochs'):\n    model.train()  # Set the model to training mode\n    epoch_total_loss = 0\n    \n    for step, batch in enumerate(train_dataloader):\n        b_text, b_labels, b_imgs = batch\n        \n        # Tokenize text input\n        b_inputs = model.tokenizer(\n            list(b_text), truncation=True, max_length=max_seq_length,\n            return_tensors=\"pt\", padding=True\n        )\n        \n        # Move labels, images, and inputs to the GPU\n        b_labels = b_labels.to(device)\n        b_imgs = b_imgs.to(device)\n        b_inputs = {k: v.to(device) for k, v in b_inputs.items()}\n\n        # Enable mixed precision using autocast\n        with autocast():\n            b_logits = model(text=b_inputs, image=b_imgs)  # Forward pass\n            loss = criterion(b_logits, b_labels)  # Calculate loss\n        \n        # Accumulate loss for gradient accumulation\n        loss = loss / accumulation_steps\n        scaler.scale(loss).backward()  # Scale and backpropagate the loss\n        \n        # Perform optimizer step after the defined accumulation steps\n        if (step + 1) % accumulation_steps == 0 or (step + 1) == len(train_dataloader):\n            scaler.step(optimizer)  # Perform the optimizer step\n            scaler.update()  # Update the scale for next iteration\n            optimizer.zero_grad()  # Clear gradients\n            scheduler.step()  # Update learning rate at the end of each batch\n        \n        # Accumulate the total loss\n        epoch_total_loss += loss.item() * accumulation_steps\n\n    # Compute average loss for the epoch\n    avg_loss = epoch_total_loss / len(train_dataloader)\n\n    # Save the average loss and learning rate for this epoch\n    epoch_losses.append(avg_loss)\n    learning_rates.append(optimizer.param_groups[0]['lr'])\n    torch.cuda.empty_cache()  # Clear unused cached memory after each epoch\n\n\n    # Print results after each epoch\n    print(f'Epoch = {epoch_num + 1}')\n    print(f'    Epoch loss = {epoch_total_loss}')\n    print(f'    Average epoch loss = {avg_loss}')\n    print(f'    Learning rate = {optimizer.param_groups[0][\"lr\"]}')\n\nend = time.perf_counter()\nresnet_training_time = end - start\nprint(f'Training completed in {resnet_training_time} seconds')\n\n# Plot the loss and learning rate curves\nplt.figure(figsize=(12, 5))\n\n# Plot loss\nplt.subplot(1, 2, 1)\nplt.plot(range(1, num_train_epochs + 1), epoch_losses, label=\"Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss per Epoch\")\nplt.grid(True)\n\n# Plot learning rate\nplt.subplot(1, 2, 2)\nplt.plot(range(1, num_train_epochs + 1), learning_rates, label=\"Learning Rate\", color='orange')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Learning Rate\")\nplt.title(\"Learning Rate per Epoch\")\nplt.grid(True)\n\n\n","metadata":{"id":"6ybYsxKeFdaz","outputId":"133d1fc9-c213-4c8a-8c7b-e801786a4561","execution":{"iopub.status.busy":"2024-11-08T15:18:01.557978Z","iopub.execute_input":"2024-11-08T15:18:01.558329Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/3477076540.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()   # For mixed precision\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpochs:   0%|          | 0/10 [00:00<?, ?it/s]/tmp/ipykernel_30/3477076540.py:70: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nEpochs:  10%|‚ñà         | 1/10 [07:19<1:05:53, 439.27s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch = 1\n    Epoch loss = 33.54843958187848\n    Average epoch loss = 0.11035670915091607\n    Learning rate = 9.984586668665641e-06\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  20%|‚ñà‚ñà        | 2/10 [13:32<53:23, 400.45s/it]  ","output_type":"stream"},{"name":"stdout","text":"Epoch = 2\n    Epoch loss = 28.583597861230373\n    Average epoch loss = 0.0940249929645736\n    Learning rate = 9.938441702975689e-06\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  30%|‚ñà‚ñà‚ñà       | 3/10 [19:50<45:29, 389.96s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch = 3\n    Epoch loss = 23.851533415727317\n    Average epoch loss = 0.07845899149910301\n    Learning rate = 9.861849601988384e-06\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [26:05<38:25, 384.25s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch = 4\n    Epoch loss = 24.64252934232354\n    Average epoch loss = 0.08106095178395901\n    Learning rate = 9.755282581475769e-06\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [32:18<31:41, 380.24s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch = 5\n    Epoch loss = 18.76077949255705\n    Average epoch loss = 0.06171309043604292\n    Learning rate = 9.619397662556434e-06\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [38:34<25:14, 378.62s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch = 6\n    Epoch loss = 18.188053887337446\n    Average epoch loss = 0.059829124629399495\n    Learning rate = 9.45503262094184e-06\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [44:51<18:54, 378.09s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch = 7\n    Epoch loss = 17.186412513256073\n    Average epoch loss = 0.05653425168834234\n    Learning rate = 9.263200821770462e-06\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [51:09<12:36, 378.09s/it]","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#save model\nimport torch\n\n# Assuming your trained model is called 'model'\nimport os\n# Path to save the model\nmodel_save_path = 'beit_model.pth'\n\n# Save the model's state_dict\ntorch.save(model.state_dict(), model_save_path)\n\nprint(f\"Model saved to {model_save_path}\")\n","metadata":{"id":"pxLct_HGqMbg","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testing loop\n\nresnet_prediction_results = []\n\ndev_dataset = MultimodalDataset(df=df_dev, label_to_id=label_to_id, mode='dev', text_field='caption', label_field='label', image_path_field='image')\ntest_sampler = SequentialSampler(dev_dataset)\ntest_dataloader = DataLoader(dataset=dev_dataset,\n                            batch_size=batch_size,\n                            sampler=test_sampler)\n\n\nfor batch in tqdm(test_dataloader):\n\n  b_text, b_labels, b_imgs = batch\n\n  b_inputs = model.tokenizer(list(b_text), truncation=True, max_length=max_seq_length, return_tensors=\"pt\", padding=True)\n\n  b_labels = b_labels.to(device)\n  b_imgs = b_imgs.to(device)\n  b_inputs = b_inputs.to(device)\n  model.eval()\n  with torch.no_grad():\n      b_logits = model(text=b_inputs, image=b_imgs)\n      b_logits = b_logits.detach().cpu()\n\n  resnet_prediction_results += torch.argmax(b_logits, dim=-1).tolist()\n\nresnet_prediction_labels = [id_to_label[p] for p in resnet_prediction_results]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(resnet_prediction_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if (len(resnet_prediction_labels) == len(df_dev)):\n    print(True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resnet_class_report = classification_report(df_dev['label'], resnet_prediction_labels)\n\n","metadata":{"id":"E-CLXlB0Fg1m","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resnet_class_report","metadata":{"id":"vs3mO292KkiC","outputId":"96d21315-f728-4bf8-8c9c-b3d8cd040811","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_params['results']=resnet_class_report","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_params","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify the file path\nfile_path = \"/kaggle/working/training_report.txt\"\n\n# Format the dictionary as a string\nreport_content = \"\\n\".join([f\"{key}: {value}\" for key, value in training_params.items()])\n\n# Save the report to a text file\nwith open(file_path, mode=\"w\") as file:\n    file.write(report_content)\n\nprint(f\"Report saved to {file_path}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"id":"eAwbmRgCqMA7"}},{"cell_type":"markdown","source":"# Making prediction\n","metadata":{}},{"cell_type":"code","source":"df_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming df_test has columns ['caption', 'image']\n\n# Initialize the dataset for the test set (no labels, test transformation)\ntest_dataset = MultimodalDataset(\n    df=df_test,  # DataFrame containing the new data\n    label_to_id=None,  # No labels for prediction\n    mode='test',  # Since it's a test dataset, set train=False for evaluation transforms\n    text_field=\"caption\", # Column for captions\n    image_path_field=\"image\"  # Column for image paths\n)\n\n# Set up a DataLoader for the test dataset\ntest_sampler = SequentialSampler(test_dataset)\ntest_dataloader = DataLoader(\n    dataset=test_dataset,\n    batch_size=batch_size,  # Adjust your batch size as needed\n    sampler=test_sampler\n)\n\n# Now you can use the test_dataloader in a prediction loop\nprediction_results = []\n# Set the model to evaluation mode\n\nfor batch in tqdm(test_dataloader):\n    b_text, b_imgs = batch  # No labels are unpacked here\n\n    # Tokenize the input text (captions)\n    b_inputs = model.tokenizer(\n        list(b_text), truncation=True, max_length=max_seq_length, return_tensors=\"pt\", padding=True\n    )\n\n    # Move inputs to the correct device\n    b_imgs = b_imgs.to(device)\n    b_inputs = {k: v.to(device) for k, v in b_inputs.items()}\n\n    # Perform inference\n    model.eval() \n    with torch.no_grad():\n        b_logits = model(text=b_inputs, image=b_imgs)\n        b_logits = b_logits.detach().cpu()  # Move logits to CPU for further processing\n\n    # Collect predictions\n    prediction_results += torch.argmax(b_logits, dim=-1).tolist()\n\n# If you have an id_to_label mapping, convert indices to labels (if needed)\nif 'id_to_label' in locals():\n    predicted_labels = [id_to_label[p] for p in prediction_results]\nelse:\n    predicted_labels = prediction_results  # Return indices if no label mapping exists\n\n# Output predictions\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if(len(predicted_labels)==len(df_test)):\n    print(True)\nelse: print(False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the submission dictionary using index from iterrows\nprediction_results = {\n    \"results\": {\n        str(idx): predicted_labels[i] for i, (idx, row) in enumerate(df_test.iterrows())  # Map index to predicted labels\n    },\n    \"phase\": \"dev\"  # Set phase as 'test', 'dev', or 'train' as appropriate\n}\n\n# Print the structured results\nprint(prediction_results)\n","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\n# File path where you want to save the JSON file\noutput_file = \"results.json\"\n\n# Save the dictionary as a JSON file\nwith open(output_file, 'w') as f:\n    json.dump(prediction_results, f, indent=4)\n\nprint(f\"Prediction results saved to {output_file}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}