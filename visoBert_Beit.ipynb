{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["CAfBEaleFMqR"]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"904aa7e30dcb4d59b9467746b8c162d7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e959ac46311d445bbc1f378b19589f6b","IPY_MODEL_427879f091de4c6a81c7e2079584c7d6","IPY_MODEL_c1ca635e14a7454d8d8d12ca81200d9a"],"layout":"IPY_MODEL_487cb028d7504810a4438298c625bef3"}},"e959ac46311d445bbc1f378b19589f6b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9518d3fa2ced4eab980256400222b17d","placeholder":"​","style":"IPY_MODEL_651c70d8f698418694c709f966b0ee39","value":"config.json: 100%"}},"427879f091de4c6a81c7e2079584c7d6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_045d4096b3b34ffb8a689592d6848823","max":644,"min":0,"orientation":"horizontal","style":"IPY_MODEL_091a1a6434874be28374b1a35e1433b7","value":644}},"c1ca635e14a7454d8d8d12ca81200d9a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_225602a85a5b43e2b069edca5b68d165","placeholder":"​","style":"IPY_MODEL_a7ca6c9d3cc74b679af13542c98efc0b","value":" 644/644 [00:00&lt;00:00, 14.0kB/s]"}},"487cb028d7504810a4438298c625bef3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9518d3fa2ced4eab980256400222b17d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"651c70d8f698418694c709f966b0ee39":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"045d4096b3b34ffb8a689592d6848823":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"091a1a6434874be28374b1a35e1433b7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"225602a85a5b43e2b069edca5b68d165":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7ca6c9d3cc74b679af13542c98efc0b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd3ec0a42f7c483b926b95cc5420515d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b7c5d4b456ed48229ea3a9f05b88d371","IPY_MODEL_9b4197c3cf2e4ad2ac8bfdd761d986c2","IPY_MODEL_0b814eaa3e524d89af7af9651ee0adf2"],"layout":"IPY_MODEL_fa3ab71f647a40e2997408d67ab63f04"}},"b7c5d4b456ed48229ea3a9f05b88d371":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad8f02f9604642aeb821eee018224cad","placeholder":"​","style":"IPY_MODEL_3b51264cd9cc4e24b95655c120005fc6","value":"pytorch_model.bin: 100%"}},"9b4197c3cf2e4ad2ac8bfdd761d986c2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_512f9660e20d4d62a0bdbb384ecb9113","max":390403641,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f0978b748c0542aebde85092d66cf7ae","value":390403641}},"0b814eaa3e524d89af7af9651ee0adf2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb8b6b8a577e4cf09a5ff31e5157e3a3","placeholder":"​","style":"IPY_MODEL_8255833febd246d5bf682f0c6838425d","value":" 390M/390M [00:16&lt;00:00, 20.3MB/s]"}},"fa3ab71f647a40e2997408d67ab63f04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad8f02f9604642aeb821eee018224cad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b51264cd9cc4e24b95655c120005fc6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"512f9660e20d4d62a0bdbb384ecb9113":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0978b748c0542aebde85092d66cf7ae":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fb8b6b8a577e4cf09a5ff31e5157e3a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8255833febd246d5bf682f0c6838425d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"861ad6074ae24df5a1eb3b375aa10e93":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c52eae0ff2224d0b84157336d457218c","IPY_MODEL_a2a3e1c647d1478baea2ad6e55e85f04","IPY_MODEL_205d17e4bc6b42d0ab6aae5c4845eb98"],"layout":"IPY_MODEL_6c995e0e1d7149ac94c857dad8bdc3e7"}},"c52eae0ff2224d0b84157336d457218c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_27e20c4ed00843eba71e97fcff8d3288","placeholder":"​","style":"IPY_MODEL_5313d00ba1eb43e38ff9af09905b6391","value":"sentencepiece.bpe.model: 100%"}},"a2a3e1c647d1478baea2ad6e55e85f04":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d17654c9c4044888a129f507cbc4d3f1","max":470732,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d96ab2ad49d2431c95ea6733f3866232","value":470732}},"205d17e4bc6b42d0ab6aae5c4845eb98":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5620429b5e644df5bb942e87c61c5183","placeholder":"​","style":"IPY_MODEL_c54e49c1ffd547f293e195e4f6dcf3ee","value":" 471k/471k [00:00&lt;00:00, 27.0MB/s]"}},"6c995e0e1d7149ac94c857dad8bdc3e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27e20c4ed00843eba71e97fcff8d3288":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5313d00ba1eb43e38ff9af09905b6391":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d17654c9c4044888a129f507cbc4d3f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d96ab2ad49d2431c95ea6733f3866232":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5620429b5e644df5bb942e87c61c5183":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c54e49c1ffd547f293e195e4f6dcf3ee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9793698,"sourceType":"datasetVersion","datasetId":5856895},{"sourceId":9850807,"sourceType":"datasetVersion","datasetId":5879213}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport librosa\nimport pandas as pd","metadata":{"id":"hC7jAEClF1XP","execution":{"iopub.status.busy":"2024-11-08T15:17:35.419444Z","iopub.execute_input":"2024-11-08T15:17:35.420092Z","iopub.status.idle":"2024-11-08T15:17:40.498116Z","shell.execute_reply.started":"2024-11-08T15:17:35.420058Z","shell.execute_reply":"2024-11-08T15:17:40.497169Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Another version\n","metadata":{"id":"ctnXhJ28FKVJ"}},{"cell_type":"code","source":"import json\nimport os\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom transformers import AutoModel, AutoTokenizer, get_scheduler ,AutoModelForMaskedLM\nimport re\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom torch.optim import AdamW,Adam\nfrom tqdm.notebook import tqdm, trange\nfrom time import perf_counter\nfrom PIL import Image\nimport pandas as pd","metadata":{"id":"C08RsK2WIuj3","execution":{"iopub.status.busy":"2024-11-08T15:17:40.500079Z","iopub.execute_input":"2024-11-08T15:17:40.500630Z","iopub.status.idle":"2024-11-08T15:17:42.551885Z","shell.execute_reply.started":"2024-11-08T15:17:40.500586Z","shell.execute_reply":"2024-11-08T15:17:42.551119Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# set random seeds for repeatability\nimport numpy as np\nimport random\n\ndef set_seed(seed_val):\n    random.seed(seed_val)\n    np.random.seed(seed_val)\n    torch.manual_seed(seed_val)\n    torch.cuda.manual_seed_all(seed_val)\nseed_val = 0\nset_seed(seed_val)","metadata":{"id":"qxOdLLhTIxmI","execution":{"iopub.status.busy":"2024-11-08T15:17:42.552870Z","iopub.execute_input":"2024-11-08T15:17:42.553315Z","iopub.status.idle":"2024-11-08T15:17:42.564474Z","shell.execute_reply.started":"2024-11-08T15:17:42.553281Z","shell.execute_reply":"2024-11-08T15:17:42.563610Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndf=pd.read_csv('/kaggle/input/dsc-round1/train_ocr.csv')\nlen(df)","metadata":{"id":"NXlRN68ApfW-","execution":{"iopub.status.busy":"2024-11-08T15:17:42.566680Z","iopub.execute_input":"2024-11-08T15:17:42.566954Z","iopub.status.idle":"2024-11-08T15:17:42.787623Z","shell.execute_reply.started":"2024-11-08T15:17:42.566924Z","shell.execute_reply":"2024-11-08T15:17:42.786645Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"10805"},"metadata":{}}]},{"cell_type":"code","source":"import json\nimport pandas as pd\nwith open('/kaggle/input/dsc2024/vimmsd-private-test.json', 'r') as f:\n    data = json.load(f)\n\n# Sử dụng json_normalize để làm phẳng dữ liệu trong \"root\"\ndf_test = pd.DataFrame.from_dict(data, orient='index')\ndf_test.sample()","metadata":{"execution":{"iopub.status.busy":"2024-11-10T00:40:40.165106Z","iopub.execute_input":"2024-11-10T00:40:40.165970Z","iopub.status.idle":"2024-11-10T00:40:40.209718Z","shell.execute_reply.started":"2024-11-10T00:40:40.165913Z","shell.execute_reply":"2024-11-10T00:40:40.208780Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                                  image  \\\n1445  4d0a9e0270200b2fd565c3a10446f08f96149d87ac901b...   \n\n                                                caption label  \n1445  Thiếu t.iền cũng được, đừng thiếu lòng tự trọn...  None  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>caption</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1445</th>\n      <td>4d0a9e0270200b2fd565c3a10446f08f96149d87ac901b...</td>\n      <td>Thiếu t.iền cũng được, đừng thiếu lòng tự trọn...</td>\n      <td>None</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-11-10T00:41:35.058905Z","iopub.execute_input":"2024-11-10T00:41:35.059275Z","iopub.status.idle":"2024-11-10T00:41:35.064725Z","shell.execute_reply.started":"2024-11-10T00:41:35.059238Z","shell.execute_reply":"2024-11-10T00:41:35.063717Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"0\n","output_type":"stream"}]},{"cell_type":"code","source":"df['label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:17:42.826114Z","iopub.execute_input":"2024-11-08T15:17:42.826419Z","iopub.status.idle":"2024-11-08T15:17:42.840976Z","shell.execute_reply.started":"2024-11-08T15:17:42.826387Z","shell.execute_reply":"2024-11-08T15:17:42.840071Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"label\nnot-sarcasm      6062\nmulti-sarcasm    4224\nimage-sarcasm     442\ntext-sarcasm       77\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Create a copy of df for split_1\nsplit_1 = df.copy()\n\n# Create the 'not-sarcasm' column in the copy\n# Modify the 'label' column in split_1\nsplit_1['label'] = split_1['label'].apply(lambda x: 'not-sarcasm' if x == 'not-sarcasm' else 'sarcasm')\n\n# Display the first few rows to verify\nsplit_1.sample(10)\n# Display the first few rows to verify\nlen(split_1)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:17:42.842130Z","iopub.execute_input":"2024-11-08T15:17:42.842470Z","iopub.status.idle":"2024-11-08T15:17:42.859136Z","shell.execute_reply.started":"2024-11-08T15:17:42.842434Z","shell.execute_reply":"2024-11-08T15:17:42.858419Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"10805"},"metadata":{}}]},{"cell_type":"code","source":"# Filter the DataFrame for the specified labels and create a copy\nsplit_2 = df[df['label'].isin(['image-sarcasm', 'text-sarcasm', 'multi-sarcasm'])].copy()\n\n# Create a new column 'multi-sarcasm' in split_2\nsplit_2['label'] = split_2['label'].apply(lambda x: 'multi-sarcasm' if x == 'multi-sarcasm' else 'not-multi-sarcasm')\n\n# Display the first few rows to verify the new column\nsplit_2.sample(5)\nlen(split_2)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:17:42.860052Z","iopub.execute_input":"2024-11-08T15:17:42.860340Z","iopub.status.idle":"2024-11-08T15:17:42.875196Z","shell.execute_reply.started":"2024-11-08T15:17:42.860309Z","shell.execute_reply":"2024-11-08T15:17:42.874373Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"4743"},"metadata":{}}]},{"cell_type":"code","source":"# Filter the DataFrame for 'image-sarcasm' and 'text-sarcasm' and create a copy\nsplit_3 = df[df['label'].isin(['image-sarcasm', 'text-sarcasm'])].copy()\n\n# Encode 'image-sarcasm' as 1 and 'text-sarcasm' as 0\nsplit_3['label'] = split_3['label'].apply(lambda x: 'image-sarcasm' if x == 'image-sarcasm' else 'not-image-sarcasm')\n\n# Display the first few rows to verify\n(split_3.sample(5))\nlen(split_3)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:17:42.876258Z","iopub.execute_input":"2024-11-08T15:17:42.876531Z","iopub.status.idle":"2024-11-08T15:17:42.886833Z","shell.execute_reply.started":"2024-11-08T15:17:42.876501Z","shell.execute_reply":"2024-11-08T15:17:42.885865Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"519"},"metadata":{}}]},{"cell_type":"code","source":"type = 'gfdgfdg'\n# Image processing condition\nif type == 'not':\n    df_split = split_1.copy()\nelif type == 'multi':\n    df_split = split_2.copy()\nelif type == 'image':\n    df_split = split_3.copy()\nelse:\n    df_split = df.copy()\n\n# Fill NaNs with 'sarcasm'\ndf_split = df_split.applymap(lambda x: 'sarcasm' if x == ' ' else x)\n\n# Display a random sample of 20 rows\ndf_split.sample(10)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:17:42.892497Z","iopub.execute_input":"2024-11-08T15:17:42.892782Z","iopub.status.idle":"2024-11-08T15:17:42.949507Z","shell.execute_reply.started":"2024-11-08T15:17:42.892752Z","shell.execute_reply":"2024-11-08T15:17:42.948514Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/548335595.py:13: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  df_split = df_split.applymap(lambda x: 'sarcasm' if x == ' ' else x)\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"      Unnamed: 0  _key                                              image  \\\n9424        9424  9424  b93ba0882a054668e2f41d23bc5fe08c220c01571edd2c...   \n1560        1560  1560  6a0c5567b403058457571021b1140a8f8395298fc3b679...   \n9190        9190  9190  7f48657379fec679fd331f673639267455e5b104ed4c6b...   \n6775        6775  6775  2a50365927daa8d28c0d4f8d83ad500918f286a839a6fd...   \n2808        2808  2808  a91f0b7c22a49dd791417e69bb724f3186d656f5a37c39...   \n7031        7031  7031  8d9536dbc168bd90073171102f3a22377c254b0f773693...   \n2723        2723  2723  00431202d34cca500b6fa9f7976fc3bd9f6bb98d525deb...   \n4714        4714  4714  46827d1caad00dc07b80d1863c969d10f2da82620e1a61...   \n3041        3041  3041  5727101c2f08ff96e02f1de2cc92cdb1d5459c530a15ad...   \n1779        1779  1779  c90b35765c434dfefddac499b96999a4ccfe92aa501bbf...   \n\n                                                caption          label  \\\n9424                           Làm gì cũng phải có VĂN!  multi-sarcasm   \n1560  Tạm quên đi sự thất vọng và tiếc nuối sau trận...    not-sarcasm   \n9190  Xiumin (EXO) đã hạ cánh tại Sân bay nội bài tr...    not-sarcasm   \n6775  THUỲ TIÊN ĐẸP KHÔNG GÓC CHẾT BÊN CẠNH YAMAHA G...    not-sarcasm   \n2808                                             Hợp lý  multi-sarcasm   \n7031  HÀ NỘI SẮP CÓ SHOW DIỄN THỰC CẢNH TRÊN SÔNG VE...    not-sarcasm   \n2723                                   Áp lực công việc  multi-sarcasm   \n4714  Trong cặp thằng nào có 10 tờ này thì giờ kiểm ...  multi-sarcasm   \n3041                   Tây Du Ký phiên bản sắc nét 4K 😌    not-sarcasm   \n1779  Sáng nay, 6/6 hơn 96.000 học sinh tại TPHCM ch...    not-sarcasm   \n\n                                                   ocr1  \n9424  \"Nó hành tao tốn tiền quán\", \"bán mẹ đi cho kh...  \n1560  Anh Vria, 16, ĐTU22 Việt Nam, thì tích cực tập...  \n9190                                                 09  \n6775                                            sarcasm  \n2808  Làm thế nào để hô biến mức lương 5trung, 1 thà...  \n7031                                            sarcasm  \n2723  Anh xem giup em, Xem gì em, Có phải em béo lên...  \n4714  Trường :, Thứ, ngày, tháng, năm, Lớp:, KIỂM TR...  \n3041                                            sarcasm  \n1779                                            sarcasm  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>_key</th>\n      <th>image</th>\n      <th>caption</th>\n      <th>label</th>\n      <th>ocr1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>9424</th>\n      <td>9424</td>\n      <td>9424</td>\n      <td>b93ba0882a054668e2f41d23bc5fe08c220c01571edd2c...</td>\n      <td>Làm gì cũng phải có VĂN!</td>\n      <td>multi-sarcasm</td>\n      <td>\"Nó hành tao tốn tiền quán\", \"bán mẹ đi cho kh...</td>\n    </tr>\n    <tr>\n      <th>1560</th>\n      <td>1560</td>\n      <td>1560</td>\n      <td>6a0c5567b403058457571021b1140a8f8395298fc3b679...</td>\n      <td>Tạm quên đi sự thất vọng và tiếc nuối sau trận...</td>\n      <td>not-sarcasm</td>\n      <td>Anh Vria, 16, ĐTU22 Việt Nam, thì tích cực tập...</td>\n    </tr>\n    <tr>\n      <th>9190</th>\n      <td>9190</td>\n      <td>9190</td>\n      <td>7f48657379fec679fd331f673639267455e5b104ed4c6b...</td>\n      <td>Xiumin (EXO) đã hạ cánh tại Sân bay nội bài tr...</td>\n      <td>not-sarcasm</td>\n      <td>09</td>\n    </tr>\n    <tr>\n      <th>6775</th>\n      <td>6775</td>\n      <td>6775</td>\n      <td>2a50365927daa8d28c0d4f8d83ad500918f286a839a6fd...</td>\n      <td>THUỲ TIÊN ĐẸP KHÔNG GÓC CHẾT BÊN CẠNH YAMAHA G...</td>\n      <td>not-sarcasm</td>\n      <td>sarcasm</td>\n    </tr>\n    <tr>\n      <th>2808</th>\n      <td>2808</td>\n      <td>2808</td>\n      <td>a91f0b7c22a49dd791417e69bb724f3186d656f5a37c39...</td>\n      <td>Hợp lý</td>\n      <td>multi-sarcasm</td>\n      <td>Làm thế nào để hô biến mức lương 5trung, 1 thà...</td>\n    </tr>\n    <tr>\n      <th>7031</th>\n      <td>7031</td>\n      <td>7031</td>\n      <td>8d9536dbc168bd90073171102f3a22377c254b0f773693...</td>\n      <td>HÀ NỘI SẮP CÓ SHOW DIỄN THỰC CẢNH TRÊN SÔNG VE...</td>\n      <td>not-sarcasm</td>\n      <td>sarcasm</td>\n    </tr>\n    <tr>\n      <th>2723</th>\n      <td>2723</td>\n      <td>2723</td>\n      <td>00431202d34cca500b6fa9f7976fc3bd9f6bb98d525deb...</td>\n      <td>Áp lực công việc</td>\n      <td>multi-sarcasm</td>\n      <td>Anh xem giup em, Xem gì em, Có phải em béo lên...</td>\n    </tr>\n    <tr>\n      <th>4714</th>\n      <td>4714</td>\n      <td>4714</td>\n      <td>46827d1caad00dc07b80d1863c969d10f2da82620e1a61...</td>\n      <td>Trong cặp thằng nào có 10 tờ này thì giờ kiểm ...</td>\n      <td>multi-sarcasm</td>\n      <td>Trường :, Thứ, ngày, tháng, năm, Lớp:, KIỂM TR...</td>\n    </tr>\n    <tr>\n      <th>3041</th>\n      <td>3041</td>\n      <td>3041</td>\n      <td>5727101c2f08ff96e02f1de2cc92cdb1d5459c530a15ad...</td>\n      <td>Tây Du Ký phiên bản sắc nét 4K 😌</td>\n      <td>not-sarcasm</td>\n      <td>sarcasm</td>\n    </tr>\n    <tr>\n      <th>1779</th>\n      <td>1779</td>\n      <td>1779</td>\n      <td>c90b35765c434dfefddac499b96999a4ccfe92aa501bbf...</td>\n      <td>Sáng nay, 6/6 hơn 96.000 học sinh tại TPHCM ch...</td>\n      <td>not-sarcasm</td>\n      <td>sarcasm</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def text_to_emoji(text):\n    emoji_map = {\n        \":))\": \"😊\",\n        \"=)))\": \"😊\",# Happy\n        \":((\": \"😞\",  # Sad\n        \":'(\": \"😢\",  # Crying\n        \":D\": \"😃\",   # Big smile\n        \":(\": \"☹️\",   # Disappointed\n        \":|\": \"😐\",   # Neutral\n    }\n    for text_emoji, real_emoji in emoji_map.items():\n        text = text.replace(text_emoji, real_emoji)\n    return text\n\n# Preprocessing function\ndef preprocess_text(text):\n    text = text.lower()\n    text = text_to_emoji(text)\n    text = re.sub(r'[^\\w\\s,]', '', text)\n    text = re.sub(r'\\d+', '', text)\n    text = ' '.join(text.split())\n    return text\n\ndf_split['caption']=df_split['caption'].apply(preprocess_text).apply(text_to_emoji)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:17:42.950952Z","iopub.execute_input":"2024-11-08T15:17:42.951330Z","iopub.status.idle":"2024-11-08T15:17:43.357660Z","shell.execute_reply.started":"2024-11-08T15:17:42.951288Z","shell.execute_reply":"2024-11-08T15:17:43.356878Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df_split['caption']","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:17:43.358644Z","iopub.execute_input":"2024-11-08T15:17:43.358922Z","iopub.status.idle":"2024-11-08T15:17:43.366555Z","shell.execute_reply.started":"2024-11-08T15:17:43.358891Z","shell.execute_reply":"2024-11-08T15:17:43.365511Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"0                       cô ấy trên mạng vs cô ấy ngoài đời\n1               người tâm linh giao tiếp với người thực tế\n2        hình như trăng hôm nay đẹp quá mọi người ạ canhco\n3        mọi người nghĩ sao về phát biểu của shark việt...\n4                tay hai nàng chứ việc gì phải lệ hai hàng\n                               ...                        \n10800                                          lộn đầu rồi\n10801    chào các bạn, mình là goda takeshi trong live ...\n10802                                   cre hùynh quốc huy\n10803                                     anh hùng thật sự\n10804                  quá là bình thường butchr interpool\nName: caption, Length: 10805, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# split train and dev\ndf_train, df_dev= train_test_split(df_split, test_size=0.1, random_state=42)","metadata":{"id":"hAl91U8bFutY","execution":{"iopub.status.busy":"2024-11-08T15:17:43.367869Z","iopub.execute_input":"2024-11-08T15:17:43.368186Z","iopub.status.idle":"2024-11-08T15:17:43.386437Z","shell.execute_reply.started":"2024-11-08T15:17:43.368144Z","shell.execute_reply":"2024-11-08T15:17:43.385747Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, WeightedRandomSampler\nimport numpy as np\n\n# Assuming 'df' is your DataFrame and 'label' is the column with class labels\n\n# Step 1: Calculate the sampling weight for each class\nclass_counts = df_train['label'].value_counts()\nclass_weights = 1.0 / class_counts\nsample_weights = df_train['label'].map(class_weights).values  # Map the class weights to each sample","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:17:43.387363Z","iopub.execute_input":"2024-11-08T15:17:43.387609Z","iopub.status.idle":"2024-11-08T15:17:43.395734Z","shell.execute_reply.started":"2024-11-08T15:17:43.387582Z","shell.execute_reply":"2024-11-08T15:17:43.394783Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(len(df_train))\nprint(len(df_dev))\nprint(len(df_test))","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:17:43.396896Z","iopub.execute_input":"2024-11-08T15:17:43.397267Z","iopub.status.idle":"2024-11-08T15:17:43.406673Z","shell.execute_reply.started":"2024-11-08T15:17:43.397231Z","shell.execute_reply":"2024-11-08T15:17:43.405866Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"9724\n1081\n1413\n","output_type":"stream"}]},{"cell_type":"code","source":"IMAGE_TRAIN_FOLDER='/kaggle/input/dsc2024/training-images/train-images/'\nIMAGE_TEST_FOLDER='/kaggle/input/dsc2024/private-test-images/test-images/'","metadata":{"id":"DXNL5B4iFw8c","execution":{"iopub.status.busy":"2024-11-08T15:17:43.407830Z","iopub.execute_input":"2024-11-08T15:17:43.408155Z","iopub.status.idle":"2024-11-08T15:17:43.412774Z","shell.execute_reply.started":"2024-11-08T15:17:43.408124Z","shell.execute_reply":"2024-11-08T15:17:43.411923Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#Encode labels\nlabel_to_id = {lab:i for i, lab in enumerate(df_train['label'].sort_values().unique())}\nid_to_label = {v:k for k,v in label_to_id.items()}\nlabel_to_id","metadata":{"id":"HcKn4aXoF3wo","outputId":"c955d042-988a-4b37-e598-3f12f8f2164c","execution":{"iopub.status.busy":"2024-11-08T15:17:43.413840Z","iopub.execute_input":"2024-11-08T15:17:43.414738Z","iopub.status.idle":"2024-11-08T15:17:43.433132Z","shell.execute_reply.started":"2024-11-08T15:17:43.414706Z","shell.execute_reply":"2024-11-08T15:17:43.432390Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"{'image-sarcasm': 0, 'multi-sarcasm': 1, 'not-sarcasm': 2, 'text-sarcasm': 3}"},"metadata":{}}]},{"cell_type":"code","source":"num_out_labels = len(label_to_id)\nprint(\"Number of labels \", num_out_labels)","metadata":{"id":"bfKHoEsfIQkh","outputId":"446eb7d2-6328-4b43-9513-81aac38dd376","execution":{"iopub.status.busy":"2024-11-08T15:17:43.434121Z","iopub.execute_input":"2024-11-08T15:17:43.434436Z","iopub.status.idle":"2024-11-08T15:17:43.438696Z","shell.execute_reply.started":"2024-11-08T15:17:43.434404Z","shell.execute_reply":"2024-11-08T15:17:43.437794Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Number of labels  4\n","output_type":"stream"}]},{"cell_type":"code","source":"# extract layers of resnet-50 to build a new model\n\nimport torch.nn as nn\nfrom torchvision.models.resnet import resnet50\n\nclass ResNetFeatureModel(nn.Module):\n    def __init__(self, output_layer):\n        super().__init__()\n        self.output_layer = output_layer \n        pretrained_resnet = resnet50(pretrained=True)\n        self.children_list = []\n        for n,c in pretrained_resnet.named_children():\n            self.children_list.append(c)\n            if n == self.output_layer:\n                break\n\n        self.net = nn.Sequential(*self.children_list)\n\n\n    def forward(self,x):\n        x = self.net(x)\n        x = torch.flatten(x, 1)\n        return x","metadata":{"id":"jZS6yB6dFXQC","execution":{"iopub.status.busy":"2024-11-08T15:17:43.439918Z","iopub.execute_input":"2024-11-08T15:17:43.440716Z","iopub.status.idle":"2024-11-08T15:17:43.447893Z","shell.execute_reply.started":"2024-11-08T15:17:43.440666Z","shell.execute_reply":"2024-11-08T15:17:43.446976Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import BeitModel, BeitConfig\n\nclass BeitFeatureModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Load pre-trained BEiT model\n        self.beit = BeitModel.from_pretrained(\"microsoft/beit-base-patch16-224\")\n\n    def forward(self, x):\n        # Extract features for the image\n        outputs = self.beit(pixel_values=x)\n        # Get the [CLS] token representation\n        img_features = outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, hidden_size)\n        return img_features\n","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:17:43.449004Z","iopub.execute_input":"2024-11-08T15:17:43.449365Z","iopub.status.idle":"2024-11-08T15:17:43.495767Z","shell.execute_reply.started":"2024-11-08T15:17:43.449324Z","shell.execute_reply":"2024-11-08T15:17:43.495090Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom PIL import Image\nfrom torchvision import transforms\n\nclass MultimodalDataset(Dataset):\n    def __init__(self, df, label_to_id=None, mode='train', text_field=\"caption\", label_field='label', image_path_field=\"image\"):\n        \"\"\"\n        Args:\n            df (DataFrame): The DataFrame containing your data.\n            label_to_id (dict): Dictionary for mapping labels to IDs. Set to None for test data.\n            mode (str): Mode of the dataset. One of ['train', 'test', 'dev'].\n            text_field (str): Column name for text data.\n            label_field (str): Column name for label data.\n            image_path_field (str): Column name for image paths.\n        \"\"\"\n        self.df = df.reset_index(drop=True)\n        self.label_to_id = label_to_id\n        self.mode = mode  # Mode can be 'train', 'test', or 'dev'\n        self.text_field = text_field\n        #self.ocr=ocr_field\n        self.label_field = label_field\n        self.image_path_field = image_path_field\n\n        # ResNet-50 settings\n        self.img_size = 224\n        self.mean, self.std = (\n            0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)\n\n        # Define different transformations based on the mode\n        self.train_transform_func = transforms.Compose([\n            transforms.RandomResizedCrop(self.img_size, scale=(0.5, 1.0)),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize(self.mean, self.std)\n        ])\n\n        self.test_transform_func = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(self.img_size),\n            transforms.ToTensor(),\n            transforms.Normalize(self.mean, self.std)\n        ])\n\n        self.dev_transform_func = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(self.img_size),\n            transforms.ToTensor(),\n            transforms.Normalize(self.mean, self.std)\n        ])\n\n    def __getitem__(self, index):\n        # Get text data\n        text = str(self.df.at[index, self.text_field])\n        \n        # Select the correct image folder based on mode\n        if self.mode == 'test':\n            img_path = IMAGE_TEST_FOLDER + self.df.at[index, self.image_path_field]\n        else:\n            img_path = IMAGE_TRAIN_FOLDER + self.df.at[index, self.image_path_field]\n\n        # Load the image\n        image = Image.open(img_path).convert('RGB')  # Ensure the image is in RGB format\n\n        # Apply appropriate transformations based on mode\n        if self.mode == 'train':\n            img = self.train_transform_func(image)\n        elif self.mode == 'test':\n            img = self.test_transform_func(image)\n        elif self.mode == 'dev':\n            img = self.dev_transform_func(image)\n\n        # If labels are available, return them, else only return the image and text\n        if self.label_to_id is not None and self.label_field in self.df.columns:\n            label = self.label_to_id[self.df.at[index, self.label_field]]\n            return text, label, img\n        else:\n            return text, img\n\n    def __len__(self):\n        return self.df.shape[0]\n","metadata":{"id":"gqcCN29AFL-t","execution":{"iopub.status.busy":"2024-11-08T15:17:43.496947Z","iopub.execute_input":"2024-11-08T15:17:43.497249Z","iopub.status.idle":"2024-11-08T15:17:43.512657Z","shell.execute_reply.started":"2024-11-08T15:17:43.497191Z","shell.execute_reply":"2024-11-08T15:17:43.511757Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"class VisoBertBeitModel(nn.Module):\n    def __init__(self, num_labels, text_pretrained='uitnlp/visobert', mlp_hidden_size=512, dropout_prob=0.3):\n        super().__init__()\n        # Text encoder (ViSoBERT)\n        self.text_encoder = AutoModel.from_pretrained(text_pretrained)\n        self.tokenizer = AutoTokenizer.from_pretrained(text_pretrained)\n\n        # Visual encoder (BEiT)\n        self.visual_encoder = BeitFeatureModel()\n        \n        # Check hidden sizes\n        self.text_hidden_size = self.text_encoder.config.hidden_size\n        self.image_hidden_size = self.visual_encoder.beit.config.hidden_size\n\n        # MLP with one hidden layer\n        self.mlp = nn.Sequential(\n            nn.Linear(self.text_hidden_size + self.image_hidden_size, mlp_hidden_size),\n            nn.ReLU(),\n            nn.Dropout(dropout_prob),\n        )\n\n        # Classifier layer\n        self.classifier = nn.Linear(mlp_hidden_size, num_labels)\n\n    def forward(self, text, image):\n        # Encode text and extract the [CLS] token feature\n        text_output = self.text_encoder(**text)\n        text_feature = text_output.last_hidden_state[:, 0, :]  # Shape: (batch_size, text_hidden_size)\n\n        # Encode image features\n        img_feature = self.visual_encoder(image)  # Shape: (batch_size, image_hidden_size)\n        \n        # Concatenate text and image features\n        features = torch.cat((text_feature, img_feature), dim=1)  # Shape: (batch_size, text_hidden_size + image_hidden_size)\n\n        # Pass through the MLP layer\n        mlp_output = self.mlp(features)\n\n        # Classify using the final output of the MLP\n        logits = self.classifier(mlp_output)\n\n        return logits","metadata":{"id":"xZHp2kC5FZ6v","execution":{"iopub.status.busy":"2024-11-08T15:17:43.514075Z","iopub.execute_input":"2024-11-08T15:17:43.514407Z","iopub.status.idle":"2024-11-08T15:17:43.524804Z","shell.execute_reply.started":"2024-11-08T15:17:43.514376Z","shell.execute_reply":"2024-11-08T15:17:43.524034Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torch.utils.data import DataLoader, WeightedRandomSampler\n\ndef create_oversampled_dataloader(dataset, label_col, batch_size, num_workers=4):\n    # Đếm số lượng mẫu của mỗi class trong dataset ban đầu\n    class_sample_counts = dataset.df[label_col].value_counts().to_dict()\n    \n    # Tính trọng số cho mỗi sample dựa trên số lượng mẫu của class đó\n    weights = 1. / np.array([class_sample_counts[label] for label in dataset.df[label_col]])\n    \n    # In thông tin về dataset trước khi oversampling\n    print(\"Số lượng mẫu của mỗi class trước khi oversampling:\", class_sample_counts)\n\n    # Tạo WeightedRandomSampler với trọng số đã tính\n    sampler = WeightedRandomSampler(weights, num_samples=len(dataset), replacement=True)\n\n    # Tạo DataLoader với sampler\n    dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler, num_workers=num_workers)\n\n    # Tính alpha trước khi oversampling\n    total_count_before = sum(class_sample_counts.values())\n    alpha_before_oversampling = {class_label: count / total_count_before for class_label, count in class_sample_counts.items()}\n    print(\"Alpha trước khi oversampling:\", alpha_before_oversampling)\n\n    # Tính số lượng mẫu sau khi oversampling\n    class_counts_after_sampling = {i: 0 for i in range(len(class_sample_counts))}\n    \n    for index in sampler:\n        label = dataset[index][2]  # Index 2 là nhãn (label)\n        class_counts_after_sampling[label] += 1\n    \n    # In thông tin về dataset sau khi oversampling\n    print(\"Số lượng mẫu của mỗi class sau khi oversampling:\", class_counts_after_sampling)\n\n    # Tính alpha sau khi oversampling\n    total_count_after = sum(class_counts_after_sampling.values())\n    alpha_after_oversampling = {class_label: count / total_count_after for class_label, count in class_counts_after_sampling.items()}\n    print(\"Alpha sau khi oversampling:\", alpha_after_oversampling)\n\n    # Chuyển alpha thành tensor để sử dụng trong Focal Loss\n    alpha_for_focal = torch.tensor([alpha_after_oversampling[i] for i in range(len(alpha_after_oversampling))], dtype=torch.float32).to('cuda' if torch.cuda.is_available() else 'cpu')\n\n    return dataloader, alpha_for_focal\n","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:17:43.526105Z","iopub.execute_input":"2024-11-08T15:17:43.526471Z","iopub.status.idle":"2024-11-08T15:17:43.540141Z","shell.execute_reply.started":"2024-11-08T15:17:43.526432Z","shell.execute_reply":"2024-11-08T15:17:43.539239Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"len(label_to_id)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:17:43.541295Z","iopub.execute_input":"2024-11-08T15:17:43.542067Z","iopub.status.idle":"2024-11-08T15:17:43.552634Z","shell.execute_reply.started":"2024-11-08T15:17:43.542034Z","shell.execute_reply":"2024-11-08T15:17:43.551761Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"4"},"metadata":{}}]},{"cell_type":"code","source":"import torch\n\n# Specify the path to the saved model\nmodel_path = \"/kaggle/input/dsc2024/resnet_model_v8.pth\"\n\n# Assuming the same model architecture is defined\nmodel = VisoBertBeitModel(num_labels=num_out_labels, text_pretrained='uitnlp/visobert')\n#model = VisoBertResNetModel(num_labels=len(label_to_id), text_pretrained='vinai/phobert-base')\n\n # Replace `MyModel` with your actual model class\n\n# Load the state_dict from the file\nmodel.load_state_dict(torch.load(model_path))\n\n# Move the model to the appropriate device (CPU or GPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel=model.to(device)\n\n# Set the model to evaluation mode\n#model.train()\n\n#print(f\"Model loaded from {model_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:17:43.553722Z","iopub.execute_input":"2024-11-08T15:17:43.554000Z","iopub.status.idle":"2024-11-08T15:17:58.003758Z","shell.execute_reply.started":"2024-11-08T15:17:43.553970Z","shell.execute_reply":"2024-11-08T15:17:58.002260Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/644 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1ab28e0ce46494b9a95f6d2142d07cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/390M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"458b1f1d77814fc084736d4f0ce369c0"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaModel were not initialized from the model checkpoint at uitnlp/visobert and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/471k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff5e380a47c14fb5ae0b0a017b2c0743"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/69.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0d3c1694f7a422eab4bca219f58b702"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/350M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd7f920c108b470ebabb9b707dbe7459"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_30/849942651.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path))\n","output_type":"stream"}]},{"cell_type":"code","source":"len(df_train)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:17:58.007086Z","iopub.execute_input":"2024-11-08T15:17:58.008074Z","iopub.status.idle":"2024-11-08T15:17:58.014893Z","shell.execute_reply.started":"2024-11-08T15:17:58.008028Z","shell.execute_reply":"2024-11-08T15:17:58.013897Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"9724"},"metadata":{}}]},{"cell_type":"code","source":"!nvidia-smi -L","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:17:58.016076Z","iopub.execute_input":"2024-11-08T15:17:58.016454Z","iopub.status.idle":"2024-11-08T15:18:00.518823Z","shell.execute_reply.started":"2024-11-08T15:17:58.016415Z","shell.execute_reply":"2024-11-08T15:18:00.517625Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"GPU 0: Tesla T4 (UUID: GPU-2d2aefef-4c29-4559-6c8f-6794e2121533)\nGPU 1: Tesla T4 (UUID: GPU-7e685d48-1d93-5c8a-6320-84cb3f574da4)\n","output_type":"stream"}]},{"cell_type":"code","source":"# parameters\ntraining_params = {\n    \"seed_val\": seed_val,\n    \"training_size\" : len(df_train),\n    \"dev_size\": len(df_dev),\n    \"test_size\": len(df_test),\n    \"num_train_epochs\": 30,\n    \"batch_size\": 16,\n    \"learning_rate\": 1e-5, \n    \"weight_decay\": 0.01,\n    \"warmup_steps\": 0,\n    \"max_seq_length\": 512\n}\n","metadata":{"id":"WlAYOob2Fqqq","execution":{"iopub.status.busy":"2024-11-08T15:18:00.527266Z","iopub.execute_input":"2024-11-08T15:18:00.527671Z","iopub.status.idle":"2024-11-08T15:18:00.538288Z","shell.execute_reply.started":"2024-11-08T15:18:00.527626Z","shell.execute_reply":"2024-11-08T15:18:00.533226Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Accessing each value by key\nseed_val = training_params['seed_val']\ntraining_size = training_params['training_size']\ndev_size = training_params['dev_size']\ntest_size = training_params['test_size']\nnum_train_epochs = training_params['num_train_epochs']\nbatch_size = training_params['batch_size']\nlearning_rate = training_params['learning_rate']\nweight_decay = training_params['weight_decay']\nwarmup_steps = training_params['warmup_steps']\nmax_seq_length = training_params['max_seq_length']\n","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:18:00.539547Z","iopub.execute_input":"2024-11-08T15:18:00.539826Z","iopub.status.idle":"2024-11-08T15:18:01.538315Z","shell.execute_reply.started":"2024-11-08T15:18:00.539796Z","shell.execute_reply":"2024-11-08T15:18:01.537283Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"training_params","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:18:01.539624Z","iopub.execute_input":"2024-11-08T15:18:01.539937Z","iopub.status.idle":"2024-11-08T15:18:01.549959Z","shell.execute_reply.started":"2024-11-08T15:18:01.539905Z","shell.execute_reply":"2024-11-08T15:18:01.549176Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"{'seed_val': 0,\n 'training_size': 9724,\n 'dev_size': 1081,\n 'test_size': 1413,\n 'num_train_epochs': 10,\n 'batch_size': 32,\n 'learning_rate': 1e-05,\n 'weight_decay': 0.01,\n 'warmup_steps': 0,\n 'max_seq_length': 512}"},"metadata":{}}]},{"cell_type":"code","source":"\n\n# Step 2: Create the WeightedRandomSampler\n#sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-08T15:18:01.551930Z","iopub.execute_input":"2024-11-08T15:18:01.552737Z","iopub.status.idle":"2024-11-08T15:18:01.556773Z","shell.execute_reply.started":"2024-11-08T15:18:01.552694Z","shell.execute_reply":"2024-11-08T15:18:01.555828Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# training step\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.utils.data import DataLoader, RandomSampler\nfrom tqdm import trange, tqdm\nimport torch.nn as nn\nfrom transformers import get_scheduler, AdamW\nimport time\n\n\n# Set up gradient accumulation steps and use mixed precision\naccumulation_steps = 4  # Perform backward pass and optimizer step after this many batches\nscaler = GradScaler()   # For mixed precision\n\ntrain_dataset = MultimodalDataset(df=df_train, label_to_id=label_to_id, mode='train', text_field='caption', label_field='label', image_path_field='image')\ntrain_sampler = RandomSampler(train_dataset)\ntrain_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, sampler=train_sampler)\n#train_dataloader, class_weights = create_oversampled_dataloader(train_dataset, label_col='label', batch_size=batch_size)  # Tăng num_workers để tải dữ liệu song song\n\n# Step 3: Use the sampler in the DataLoader\n#train_dataloader = DataLoader(\n #   df_train,        # Replace with your dataset object\n  #  batch_size=batch_size,       # Set batch size as desired\n   # sampler=sampler      # Pass the sampler\n#)\nt_total = len(train_dataloader) * num_train_epochs\n\noptimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\nscheduler = get_scheduler(name=\"cosine\", optimizer=optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n\n# Set the device to the second GPU (GPU 1)\n# Move model to the device\ncriterion = nn.CrossEntropyLoss()\n#criterion = L.CrossEntropyFocalLoss(gamma=1.0, reduction='mean', class_weights=class_weights)\n\nstart = time.perf_counter()\n\n#for param in model.text_encoder.parameters():\n #   param.requires_grad = True\n\n#for param in model.visual_encoder.parameters():\n #   param.requires_grad = True\n\n\n# Initialize lists to store epoch loss and learning rate\nepoch_losses = []\nlearning_rates = []\n\n# Set up training loop with tqdm only for epochs\nfor epoch_num in trange(num_train_epochs, desc='Epochs'):\n    model.train()  # Set the model to training mode\n    epoch_total_loss = 0\n    \n    for step, batch in enumerate(train_dataloader):\n        b_text, b_labels, b_imgs = batch\n        \n        # Tokenize text input\n        b_inputs = model.tokenizer(\n            list(b_text), truncation=True, max_length=max_seq_length,\n            return_tensors=\"pt\", padding=True\n        )\n        \n        # Move labels, images, and inputs to the GPU\n        b_labels = b_labels.to(device)\n        b_imgs = b_imgs.to(device)\n        b_inputs = {k: v.to(device) for k, v in b_inputs.items()}\n\n        # Enable mixed precision using autocast\n        with autocast():\n            b_logits = model(text=b_inputs, image=b_imgs)  # Forward pass\n            loss = criterion(b_logits, b_labels)  # Calculate loss\n        \n        # Accumulate loss for gradient accumulation\n        loss = loss / accumulation_steps\n        scaler.scale(loss).backward()  # Scale and backpropagate the loss\n        \n        # Perform optimizer step after the defined accumulation steps\n        if (step + 1) % accumulation_steps == 0 or (step + 1) == len(train_dataloader):\n            scaler.step(optimizer)  # Perform the optimizer step\n            scaler.update()  # Update the scale for next iteration\n            optimizer.zero_grad()  # Clear gradients\n            scheduler.step()  # Update learning rate at the end of each batch\n        \n        # Accumulate the total loss\n        epoch_total_loss += loss.item() * accumulation_steps\n\n    # Compute average loss for the epoch\n    avg_loss = epoch_total_loss / len(train_dataloader)\n\n    # Save the average loss and learning rate for this epoch\n    epoch_losses.append(avg_loss)\n    learning_rates.append(optimizer.param_groups[0]['lr'])\n    torch.cuda.empty_cache()  # Clear unused cached memory after each epoch\n\n\n    # Print results after each epoch\n    print(f'Epoch = {epoch_num + 1}')\n    print(f'    Epoch loss = {epoch_total_loss}')\n    print(f'    Average epoch loss = {avg_loss}')\n    print(f'    Learning rate = {optimizer.param_groups[0][\"lr\"]}')\n\nend = time.perf_counter()\nresnet_training_time = end - start\nprint(f'Training completed in {resnet_training_time} seconds')\n\n# Plot the loss and learning rate curves\nplt.figure(figsize=(12, 5))\n\n# Plot loss\nplt.subplot(1, 2, 1)\nplt.plot(range(1, num_train_epochs + 1), epoch_losses, label=\"Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss per Epoch\")\nplt.grid(True)\n\n# Plot learning rate\nplt.subplot(1, 2, 2)\nplt.plot(range(1, num_train_epochs + 1), learning_rates, label=\"Learning Rate\", color='orange')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Learning Rate\")\nplt.title(\"Learning Rate per Epoch\")\nplt.grid(True)\n\n\n","metadata":{"id":"6ybYsxKeFdaz","outputId":"133d1fc9-c213-4c8a-8c7b-e801786a4561","execution":{"iopub.status.busy":"2024-11-08T15:18:01.557978Z","iopub.execute_input":"2024-11-08T15:18:01.558329Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/3477076540.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()   # For mixed precision\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpochs:   0%|          | 0/10 [00:00<?, ?it/s]/tmp/ipykernel_30/3477076540.py:70: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nEpochs:  10%|█         | 1/10 [07:19<1:05:53, 439.27s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch = 1\n    Epoch loss = 33.54843958187848\n    Average epoch loss = 0.11035670915091607\n    Learning rate = 9.984586668665641e-06\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  20%|██        | 2/10 [13:32<53:23, 400.45s/it]  ","output_type":"stream"},{"name":"stdout","text":"Epoch = 2\n    Epoch loss = 28.583597861230373\n    Average epoch loss = 0.0940249929645736\n    Learning rate = 9.938441702975689e-06\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  30%|███       | 3/10 [19:50<45:29, 389.96s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch = 3\n    Epoch loss = 23.851533415727317\n    Average epoch loss = 0.07845899149910301\n    Learning rate = 9.861849601988384e-06\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  40%|████      | 4/10 [26:05<38:25, 384.25s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch = 4\n    Epoch loss = 24.64252934232354\n    Average epoch loss = 0.08106095178395901\n    Learning rate = 9.755282581475769e-06\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  50%|█████     | 5/10 [32:18<31:41, 380.24s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch = 5\n    Epoch loss = 18.76077949255705\n    Average epoch loss = 0.06171309043604292\n    Learning rate = 9.619397662556434e-06\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  60%|██████    | 6/10 [38:34<25:14, 378.62s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch = 6\n    Epoch loss = 18.188053887337446\n    Average epoch loss = 0.059829124629399495\n    Learning rate = 9.45503262094184e-06\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  70%|███████   | 7/10 [44:51<18:54, 378.09s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch = 7\n    Epoch loss = 17.186412513256073\n    Average epoch loss = 0.05653425168834234\n    Learning rate = 9.263200821770462e-06\n","output_type":"stream"},{"name":"stderr","text":"Epochs:  80%|████████  | 8/10 [51:09<12:36, 378.09s/it]","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#save model\nimport torch\n\n# Assuming your trained model is called 'model'\nimport os\n# Path to save the model\nmodel_save_path = 'beit_model.pth'\n\n# Save the model's state_dict\ntorch.save(model.state_dict(), model_save_path)\n\nprint(f\"Model saved to {model_save_path}\")\n","metadata":{"id":"pxLct_HGqMbg","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testing loop\n\nresnet_prediction_results = []\n\ndev_dataset = MultimodalDataset(df=df_dev, label_to_id=label_to_id, mode='dev', text_field='caption', label_field='label', image_path_field='image')\ntest_sampler = SequentialSampler(dev_dataset)\ntest_dataloader = DataLoader(dataset=dev_dataset,\n                            batch_size=batch_size,\n                            sampler=test_sampler)\n\n\nfor batch in tqdm(test_dataloader):\n\n  b_text, b_labels, b_imgs = batch\n\n  b_inputs = model.tokenizer(list(b_text), truncation=True, max_length=max_seq_length, return_tensors=\"pt\", padding=True)\n\n  b_labels = b_labels.to(device)\n  b_imgs = b_imgs.to(device)\n  b_inputs = b_inputs.to(device)\n  model.eval()\n  with torch.no_grad():\n      b_logits = model(text=b_inputs, image=b_imgs)\n      b_logits = b_logits.detach().cpu()\n\n  resnet_prediction_results += torch.argmax(b_logits, dim=-1).tolist()\n\nresnet_prediction_labels = [id_to_label[p] for p in resnet_prediction_results]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(resnet_prediction_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if (len(resnet_prediction_labels) == len(df_dev)):\n    print(True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resnet_class_report = classification_report(df_dev['label'], resnet_prediction_labels)\n\n","metadata":{"id":"E-CLXlB0Fg1m","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resnet_class_report","metadata":{"id":"vs3mO292KkiC","outputId":"96d21315-f728-4bf8-8c9c-b3d8cd040811","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_params['results']=resnet_class_report","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_params","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify the file path\nfile_path = \"/kaggle/working/training_report.txt\"\n\n# Format the dictionary as a string\nreport_content = \"\\n\".join([f\"{key}: {value}\" for key, value in training_params.items()])\n\n# Save the report to a text file\nwith open(file_path, mode=\"w\") as file:\n    file.write(report_content)\n\nprint(f\"Report saved to {file_path}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"id":"eAwbmRgCqMA7"}},{"cell_type":"markdown","source":"# Making prediction\n","metadata":{}},{"cell_type":"code","source":"df_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming df_test has columns ['caption', 'image']\n\n# Initialize the dataset for the test set (no labels, test transformation)\ntest_dataset = MultimodalDataset(\n    df=df_test,  # DataFrame containing the new data\n    label_to_id=None,  # No labels for prediction\n    mode='test',  # Since it's a test dataset, set train=False for evaluation transforms\n    text_field=\"caption\", # Column for captions\n    image_path_field=\"image\"  # Column for image paths\n)\n\n# Set up a DataLoader for the test dataset\ntest_sampler = SequentialSampler(test_dataset)\ntest_dataloader = DataLoader(\n    dataset=test_dataset,\n    batch_size=batch_size,  # Adjust your batch size as needed\n    sampler=test_sampler\n)\n\n# Now you can use the test_dataloader in a prediction loop\nprediction_results = []\n# Set the model to evaluation mode\n\nfor batch in tqdm(test_dataloader):\n    b_text, b_imgs = batch  # No labels are unpacked here\n\n    # Tokenize the input text (captions)\n    b_inputs = model.tokenizer(\n        list(b_text), truncation=True, max_length=max_seq_length, return_tensors=\"pt\", padding=True\n    )\n\n    # Move inputs to the correct device\n    b_imgs = b_imgs.to(device)\n    b_inputs = {k: v.to(device) for k, v in b_inputs.items()}\n\n    # Perform inference\n    model.eval() \n    with torch.no_grad():\n        b_logits = model(text=b_inputs, image=b_imgs)\n        b_logits = b_logits.detach().cpu()  # Move logits to CPU for further processing\n\n    # Collect predictions\n    prediction_results += torch.argmax(b_logits, dim=-1).tolist()\n\n# If you have an id_to_label mapping, convert indices to labels (if needed)\nif 'id_to_label' in locals():\n    predicted_labels = [id_to_label[p] for p in prediction_results]\nelse:\n    predicted_labels = prediction_results  # Return indices if no label mapping exists\n\n# Output predictions\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if(len(predicted_labels)==len(df_test)):\n    print(True)\nelse: print(False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the submission dictionary using index from iterrows\nprediction_results = {\n    \"results\": {\n        str(idx): predicted_labels[i] for i, (idx, row) in enumerate(df_test.iterrows())  # Map index to predicted labels\n    },\n    \"phase\": \"dev\"  # Set phase as 'test', 'dev', or 'train' as appropriate\n}\n\n# Print the structured results\nprint(prediction_results)\n","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\n# File path where you want to save the JSON file\noutput_file = \"results.json\"\n\n# Save the dictionary as a JSON file\nwith open(output_file, 'w') as f:\n    json.dump(prediction_results, f, indent=4)\n\nprint(f\"Prediction results saved to {output_file}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}